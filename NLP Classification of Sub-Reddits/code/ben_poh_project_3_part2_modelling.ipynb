{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d38742fd",
   "metadata": {},
   "source": [
    " **Project 3: Classification of Sub-Reddits with NLP**\n",
    " ***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ee5585",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba39e81",
   "metadata": {},
   "source": [
    "In this section, I aim to train a few models to perform predictions on this binary classification problem. The process is as follows:\n",
    "\n",
    "**Lemmatization $\\rightarrow$ Vectorization $\\rightarrow$ Modelling (train dataset) $\\rightarrow$ Predicting**\n",
    "\n",
    "It is good practice to tokenize or lemmatize before using sklearn's vectorization tools (`Count/TF-IDF Vectorizer`) otherwise a naive splitting by spaces will be applied sklearn. I have used nltk's `WordNetLemmatizer` for this project.\n",
    "\n",
    "Very broadly, I have trained several models (Logistic Regression, Random Forest and Multinomial Naive Bayes) and tuned the parameters with `GridSearchCV`. The best model is **Count Vectorization + Multinomial Naive Bayes (with lemmatization)**. I will further explain my methodology and results in the sections below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a97995",
   "metadata": {},
   "source": [
    "## Import Libraries and Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18491ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, mean_squared_error, accuracy_score, precision_score, recall_score, f1_score, plot_roc_curve, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3913419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bde013dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = pd.read_csv('../datasets/clean_problems.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80487cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "success = pd.read_csv('../datasets/clean_success.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e2fa968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1241, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problems.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb6b5c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1451, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "success.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a39a905",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99eb1e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(model, X_test, y_test):\n",
    "    '''Function to print model results in a dataframe'''\n",
    "    \n",
    "    best_score = np.round(model.best_score_,5)\n",
    "    \n",
    "    accuracy = np.round(accuracy_score(y_test, model.predict(X_test)),5)\n",
    "    precision = np.round(precision_score(y_test, model.predict(X_test)),5)\n",
    "    recall = np.round(recall_score(y_test, model.predict(X_test)),5)\n",
    "    \n",
    "    f1 = np.round(f1_score(y_test, model.predict(X_test)),5)\n",
    "    \n",
    "    best_params = model.best_params_\n",
    "    \n",
    "    summary = {'Best Score': best_score,\n",
    "               'Test Score (accuracy)': accuracy,\n",
    "               'Precision': precision,\n",
    "               'Recall': recall,\n",
    "               'F1': f1,\n",
    "              'Best Params': best_params}\n",
    "    \n",
    "    summary_df = pd.DataFrame.from_dict(summary, orient='Index', columns=[str(model.estimator.steps[0][0])+ ' x ' + str(model.estimator.steps[1][0])])\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab55d6b",
   "metadata": {},
   "source": [
    "## Define Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac1520a",
   "metadata": {},
   "source": [
    "These are the same 326 stop words as part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af7be3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stop_words = ['success', 'problem', 'successes', 'problems', 'british', 've', 'wa', 'ha']\n",
    "total_stop_words = text.ENGLISH_STOP_WORDS.union(new_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f3d0f945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_stop_words )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721d1931",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18e8468",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cc9c7d",
   "metadata": {},
   "source": [
    "Lemmatization will be applied to all posts in the sub-reddit. In this case, all the posts in the two sub-reddits will be known as the **corpus**. I wrote a function `lemmatize` to perform the lemmatization per post. This is then applied to the entire corpus using a `for` loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a71e6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(content):\n",
    "    '''Function to lemmatize a document'''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    words = content.lower().split()\n",
    "    lem_words = [lemmatizer.lemmatize(i) for i in words]\n",
    "    \n",
    "    return(' '.join(lem_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f6ba1d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatize all the posts in 'Problems' and 'Success'\n",
    "\n",
    "lem_problems_posts = []\n",
    "lem_success_posts = []\n",
    "\n",
    "for problems_post in problems['content']:\n",
    "    lem_problems_posts.append(lemmatize(problems_post))\n",
    "    \n",
    "for success_post in success['content']:\n",
    "    lem_success_posts.append(lemmatize(success_post))\n",
    "\n",
    "problems['content_lem'] = pd.DataFrame(lem_problems_posts)\n",
    "success['content_lem'] = pd.DataFrame(lem_success_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baf90a2",
   "metadata": {},
   "source": [
    "## Label and Concate Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968797df",
   "metadata": {},
   "source": [
    "Encoded British Problems subreddit with '1' and British Success subreddit with '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d18fddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "problems['label'] = 1\n",
    "success['label'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c4a7f9",
   "metadata": {},
   "source": [
    "Combined both subreddits into one dataset titled '**problems_success**'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b4d3607",
   "metadata": {},
   "outputs": [],
   "source": [
    "problems_success = pd.concat([problems, success], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fb1f0dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2692, 9)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problems_success.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65358d69",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4f8086",
   "metadata": {},
   "source": [
    "I would broadly categorise my modelling process into 3 parts:\n",
    "- **Part 1**: Lemmatization with CountVectorizer x Logistic Regression / Random Forest / Multinomial Naive Bayes\n",
    "- **Part 2**: Lemmatization with TF-IDF Vectorizer x Logistic Regression / Random Forest / Multinomial Naive Bayes\n",
    "- **Part 3**: CountVectorizer/TF-IDF Vectorizer x Random Forest / Multinomial Naive Bayes\n",
    "\n",
    "The best model I found is the **Lemmatization with CountVectorizer x Multinomial Naive Bayes** as it has the best accuracy score (0.77) across all models that I tested. Similarly, it has the best F1 score and fairly balanced precision and recall scores.\n",
    "\n",
    "The difference in Part 1 and Part 2 is the vectorization tool used (CountVectorizer vs TF-IDF Vectorizer). Compared to CountVectorizer, TF-IDF will increase weights on less common words (since they potentially have better predictive value). After completing Part 1 and 2, I removed the lemmatization step to see if it improves my model (it did not).\n",
    "\n",
    "In all 3 parts of the modelling section, I have used GridSearchCV to tune the hyperparameters of the model I am using. In general, these are the parameters I used in my models (with specific tuning based on each model):\n",
    "    \n",
    "    'cvec/tvec__max_features': [2000, 3000, 4000],\n",
    "    'cvec/tvec__min_df': [2, 3],\n",
    "    'cvec/tvec__max_df': [0.9, 0.95],\n",
    "    'cvec/tvec__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'cvec/tvec__stop_words': [total_stop_words],\n",
    "\n",
    "More details will be explained in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2401c990",
   "metadata": {},
   "source": [
    "## Define and Split Train/Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de88026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = problems_success['content_lem']\n",
    "y = problems_success['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bef59105",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59512a69",
   "metadata": {},
   "source": [
    "## Baseline Accruracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625e9685",
   "metadata": {},
   "source": [
    "Since the number of posts in both datasets are fairly close, we have a baseline accuracy of **46%** for 1 ('problems'). We compare our models against the baseline accuracy to see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cc40e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.539004\n",
       "1    0.460996\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cc42d4",
   "metadata": {},
   "source": [
    "## Part 1: Lemmatization & Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e38b3f",
   "metadata": {},
   "source": [
    "### Count Vectorizer x Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afcd6ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.14 s, sys: 407 ms, total: 2.55 s\n",
      "Wall time: 27.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('logreg',\n",
       "                                        LogisticRegression(max_iter=10000))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'cvec__max_df': [0.9, 0.95],\n",
       "                         'cvec__max_features': [2000, 3000, 4000],\n",
       "                         'cvec__min_df': [2, 3],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       "                         'cvec__stop_words': [frozenset({'a', 'about', 'above',\n",
       "                                                         'across', 'after',\n",
       "                                                         'afterwards', 'again',\n",
       "                                                         'against', 'all',\n",
       "                                                         'almost', 'alone',\n",
       "                                                         'along', 'already',\n",
       "                                                         'also', 'although',\n",
       "                                                         'always', 'am',\n",
       "                                                         'among', 'amongst',\n",
       "                                                         'amoungst', 'amount',\n",
       "                                                         'an', 'and', 'another',\n",
       "                                                         'any', 'anyhow',\n",
       "                                                         'anyone', 'anything',\n",
       "                                                         'anyway', 'anywhere', ...})],\n",
       "                         'logreg__C': [0.1, 1, 10]})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipe_cvlogreg = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('logreg', LogisticRegression(max_iter=10000))\n",
    "])\n",
    "\n",
    "pipe_cvlogreg_params = {\n",
    "    'cvec__max_features': [2000, 3000, 4000],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [0.9, 0.95],\n",
    "    'cvec__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'cvec__stop_words': [total_stop_words],\n",
    "    'logreg__C': [0.1, 1, 10]\n",
    "    \n",
    "}\n",
    "\n",
    "gs_cvlogreg = GridSearchCV(pipe_cvlogreg, \n",
    "                  param_grid=pipe_cvlogreg_params, \n",
    "                  cv=5,\n",
    "                 n_jobs=-1)\n",
    "\n",
    "gs_cvlogreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84a1a0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cvec x logreg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Best Score</th>\n",
       "      <td>0.72463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Score (accuracy)</th>\n",
       "      <td>0.737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.72696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.6871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.70647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Best Params</th>\n",
       "      <td>{'cvec__max_df': 0.9, 'cvec__max_features': 4000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 3), 'cvec__stop_words': ('myself', 'all', 'both', 'take', 'themselves', 'hundred', 'each', 'further', 'first', 'someone', 'hasnt', 'when', 'may', 'upon', 'by', 'system', 'enough', 'too', 'they', 'therefore', 'across', 'per', 'to', 'whence', 'within', 'twenty', 'part', 'will', 'whereupon', 'everything', 'name', 'bill', 'afterwards', 'who', 'mine', 'nothing', 'give', 'some', 'thereby', 'can', 'therein', 'thin', 'something', 'get', 'it', 'beforehand', 'etc', 'back', 'always', 'amongst', 'top', 'nowhere', 'thus', 'rather', 'until', 'anyhow', 'above', 'but', 'or', 'on', 'me', 'last', 'successes', 'i', 'bottom', 'problem', 'very', 'twelve', 'still', 'toward', 'ten', 'neither', 'mill', 'them', 'wherein', 'around', 'thereupon', 've', 'thick', 'a', 'serious', 'detail', 'least', 'anyone', 'over', 'put', 'anything', 'cant', 'else', 'had', 'throughout', 'he', 'beyond', 'us', 'were', 'how', 'under', 'eg', 'few', 'interest', ...), 'logreg__C': 1}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          cvec x logreg\n",
       "Best Score             0.72463                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "Test Score (accuracy)  0.737                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "Precision              0.72696                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "Recall                 0.6871                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "F1                     0.70647                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "Best Params            {'cvec__max_df': 0.9, 'cvec__max_features': 4000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 3), 'cvec__stop_words': ('myself', 'all', 'both', 'take', 'themselves', 'hundred', 'each', 'further', 'first', 'someone', 'hasnt', 'when', 'may', 'upon', 'by', 'system', 'enough', 'too', 'they', 'therefore', 'across', 'per', 'to', 'whence', 'within', 'twenty', 'part', 'will', 'whereupon', 'everything', 'name', 'bill', 'afterwards', 'who', 'mine', 'nothing', 'give', 'some', 'thereby', 'can', 'therein', 'thin', 'something', 'get', 'it', 'beforehand', 'etc', 'back', 'always', 'amongst', 'top', 'nowhere', 'thus', 'rather', 'until', 'anyhow', 'above', 'but', 'or', 'on', 'me', 'last', 'successes', 'i', 'bottom', 'problem', 'very', 'twelve', 'still', 'toward', 'ten', 'neither', 'mill', 'them', 'wherein', 'around', 'thereupon', 've', 'thick', 'a', 'serious', 'detail', 'least', 'anyone', 'over', 'put', 'anything', 'cant', 'else', 'had', 'throughout', 'he', 'beyond', 'us', 'were', 'how', 'under', 'eg', 'few', 'interest', ...), 'logreg__C': 1}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results(gs_cvlogreg, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e655b8",
   "metadata": {},
   "source": [
    "### Count Vectorizer x Random Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31ee1ac",
   "metadata": {},
   "source": [
    "For Random Forest models, I have expanded the number of features to 5000 and 7000 since multiple hyperparameter tuning have turned up with high max_features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47477381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.98 s, sys: 453 ms, total: 7.44 s\n",
      "Wall time: 4min 38s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('rf', RandomForestClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'cvec__max_df': [0.9, 0.95],\n",
       "                         'cvec__max_features': [2000, 3000, 4000, 5000, 7000],\n",
       "                         'cvec__min_df': [2, 3],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       "                         'cvec__stop_words': [frozenset({'a', 'about', 'above',\n",
       "                                                         'across', 'after',\n",
       "                                                         'afterwards', 'again',\n",
       "                                                         'against', 'all',\n",
       "                                                         'almost', 'alone',\n",
       "                                                         'along', 'already',\n",
       "                                                         'also', 'although',\n",
       "                                                         'always', 'am',\n",
       "                                                         'among', 'amongst',\n",
       "                                                         'amoungst', 'amount',\n",
       "                                                         'an', 'and', 'another',\n",
       "                                                         'any', 'anyhow',\n",
       "                                                         'anyone', 'anything',\n",
       "                                                         'anyway', 'anywhere', ...})],\n",
       "                         'rf__min_samples_leaf': [1, 2, 3],\n",
       "                         'rf__n_estimators': [100, 150]})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipe_cvrf = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "pipe_cvrf_params = {\n",
    "    'cvec__max_features': [2000, 3000, 4000, 5000, 7000],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [.9, .95],\n",
    "    'cvec__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'cvec__stop_words': [total_stop_words],\n",
    "    'rf__n_estimators': [100, 150],\n",
    "    'rf__min_samples_leaf': [1,2,3]\n",
    "}\n",
    "\n",
    "\n",
    "gs_cvrf = GridSearchCV(pipe_cvrf, \n",
    "                  param_grid=pipe_cvrf_params,\n",
    "                  cv=5,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "gs_cvrf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e58b96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cvec x rf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Best Score</th>\n",
       "      <td>0.72214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Score (accuracy)</th>\n",
       "      <td>0.75632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.76449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.68065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.72014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Best Params</th>\n",
       "      <td>{'cvec__max_df': 0.9, 'cvec__max_features': 7000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 3), 'cvec__stop_words': ('myself', 'all', 'both', 'take', 'themselves', 'hundred', 'each', 'further', 'first', 'someone', 'hasnt', 'when', 'may', 'upon', 'by', 'system', 'enough', 'too', 'they', 'therefore', 'across', 'per', 'to', 'whence', 'within', 'twenty', 'part', 'will', 'whereupon', 'everything', 'name', 'bill', 'afterwards', 'who', 'mine', 'nothing', 'give', 'some', 'thereby', 'can', 'therein', 'thin', 'something', 'get', 'it', 'beforehand', 'etc', 'back', 'always', 'amongst', 'top', 'nowhere', 'thus', 'rather', 'until', 'anyhow', 'above', 'but', 'or', 'on', 'me', 'last', 'successes', 'i', 'bottom', 'problem', 'very', 'twelve', 'still', 'toward', 'ten', 'neither', 'mill', 'them', 'wherein', 'around', 'thereupon', 've', 'thick', 'a', 'serious', 'detail', 'least', 'anyone', 'over', 'put', 'anything', 'cant', 'else', 'had', 'throughout', 'he', 'beyond', 'us', 'were', 'how', 'under', 'eg', 'few', 'interest', ...), 'rf__min_samples_leaf': 3, 'rf__n_estimators': 150}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  cvec x rf\n",
       "Best Score             0.72214                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "Test Score (accuracy)  0.75632                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "Precision              0.76449                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "Recall                 0.68065                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "F1                     0.72014                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "Best Params            {'cvec__max_df': 0.9, 'cvec__max_features': 7000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 3), 'cvec__stop_words': ('myself', 'all', 'both', 'take', 'themselves', 'hundred', 'each', 'further', 'first', 'someone', 'hasnt', 'when', 'may', 'upon', 'by', 'system', 'enough', 'too', 'they', 'therefore', 'across', 'per', 'to', 'whence', 'within', 'twenty', 'part', 'will', 'whereupon', 'everything', 'name', 'bill', 'afterwards', 'who', 'mine', 'nothing', 'give', 'some', 'thereby', 'can', 'therein', 'thin', 'something', 'get', 'it', 'beforehand', 'etc', 'back', 'always', 'amongst', 'top', 'nowhere', 'thus', 'rather', 'until', 'anyhow', 'above', 'but', 'or', 'on', 'me', 'last', 'successes', 'i', 'bottom', 'problem', 'very', 'twelve', 'still', 'toward', 'ten', 'neither', 'mill', 'them', 'wherein', 'around', 'thereupon', 've', 'thick', 'a', 'serious', 'detail', 'least', 'anyone', 'over', 'put', 'anything', 'cant', 'else', 'had', 'throughout', 'he', 'beyond', 'us', 'were', 'how', 'under', 'eg', 'few', 'interest', ...), 'rf__min_samples_leaf': 3, 'rf__n_estimators': 150}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results(gs_cvrf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3138cd55",
   "metadata": {},
   "source": [
    "### Count Vectorizer x Mulitnomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b85443a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.15 s, sys: 90.1 ms, total: 1.24 s\n",
      "Wall time: 20.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'cvec__max_df': [0.9, 0.95],\n",
       "                         'cvec__max_features': [2000, 3000, 4000],\n",
       "                         'cvec__min_df': [2, 3],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       "                         'cvec__stop_words': [frozenset({'a', 'about', 'above',\n",
       "                                                         'across', 'after',\n",
       "                                                         'afterwards', 'again',\n",
       "                                                         'against', 'all',\n",
       "                                                         'almost', 'alone',\n",
       "                                                         'along', 'already',\n",
       "                                                         'also', 'although',\n",
       "                                                         'always', 'am',\n",
       "                                                         'among', 'amongst',\n",
       "                                                         'amoungst', 'amount',\n",
       "                                                         'an', 'and', 'another',\n",
       "                                                         'any', 'anyhow',\n",
       "                                                         'anyone', 'anything',\n",
       "                                                         'anyway', 'anywhere', ...})],\n",
       "                         'nb__alpha': [1, 2, 3]})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipe_cvnb = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe_cvnb_params = {\n",
    "    'cvec__max_features': [2000, 3000, 4000],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [.9, .95],\n",
    "    'cvec__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'cvec__stop_words': [total_stop_words],\n",
    "    'nb__alpha': [1,2,3]\n",
    "}\n",
    "\n",
    "\n",
    "gs_cvnb = GridSearchCV(pipe_cvnb, \n",
    "                  param_grid=pipe_cvnb_params,\n",
    "                  cv=5,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "gs_cvnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "273eccd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cvec x nb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Best Score</th>\n",
       "      <td>0.74096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Score (accuracy)</th>\n",
       "      <td>0.77117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.75161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.75161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.75161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Best Params</th>\n",
       "      <td>{'cvec__max_df': 0.9, 'cvec__max_features': 3000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ('myself', 'all', 'both', 'take', 'themselves', 'hundred', 'each', 'further', 'first', 'someone', 'hasnt', 'when', 'may', 'upon', 'by', 'system', 'enough', 'too', 'they', 'therefore', 'across', 'per', 'to', 'whence', 'within', 'twenty', 'part', 'will', 'whereupon', 'everything', 'name', 'bill', 'afterwards', 'who', 'mine', 'nothing', 'give', 'some', 'thereby', 'can', 'therein', 'thin', 'something', 'get', 'it', 'beforehand', 'etc', 'back', 'always', 'amongst', 'top', 'nowhere', 'thus', 'rather', 'until', 'anyhow', 'above', 'but', 'or', 'on', 'me', 'last', 'successes', 'i', 'bottom', 'problem', 'very', 'twelve', 'still', 'toward', 'ten', 'neither', 'mill', 'them', 'wherein', 'around', 'thereupon', 've', 'thick', 'a', 'serious', 'detail', 'least', 'anyone', 'over', 'put', 'anything', 'cant', 'else', 'had', 'throughout', 'he', 'beyond', 'us', 'were', 'how', 'under', 'eg', 'few', 'interest', ...), 'nb__alpha': 1}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              cvec x nb\n",
       "Best Score             0.74096                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "Test Score (accuracy)  0.77117                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "Precision              0.75161                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "Recall                 0.75161                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "F1                     0.75161                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "Best Params            {'cvec__max_df': 0.9, 'cvec__max_features': 3000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ('myself', 'all', 'both', 'take', 'themselves', 'hundred', 'each', 'further', 'first', 'someone', 'hasnt', 'when', 'may', 'upon', 'by', 'system', 'enough', 'too', 'they', 'therefore', 'across', 'per', 'to', 'whence', 'within', 'twenty', 'part', 'will', 'whereupon', 'everything', 'name', 'bill', 'afterwards', 'who', 'mine', 'nothing', 'give', 'some', 'thereby', 'can', 'therein', 'thin', 'something', 'get', 'it', 'beforehand', 'etc', 'back', 'always', 'amongst', 'top', 'nowhere', 'thus', 'rather', 'until', 'anyhow', 'above', 'but', 'or', 'on', 'me', 'last', 'successes', 'i', 'bottom', 'problem', 'very', 'twelve', 'still', 'toward', 'ten', 'neither', 'mill', 'them', 'wherein', 'around', 'thereupon', 've', 'thick', 'a', 'serious', 'detail', 'least', 'anyone', 'over', 'put', 'anything', 'cant', 'else', 'had', 'throughout', 'he', 'beyond', 'us', 'were', 'how', 'under', 'eg', 'few', 'interest', ...), 'nb__alpha': 1}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results(gs_cvnb, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa455c51",
   "metadata": {},
   "source": [
    "## Part 2: Lemmatization & TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf22b5b",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer x Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81b23c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.75 s, sys: 118 ms, total: 1.86 s\n",
      "Wall time: 30.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                       ('logreg',\n",
       "                                        LogisticRegression(max_iter=10000))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'logreg__C': [0.1, 1, 10], 'tvec__max_df': [0.9, 0.95],\n",
       "                         'tvec__max_features': [2000, 3000, 4000, 5000],\n",
       "                         'tvec__min_df': [2, 3],\n",
       "                         'tvec__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       "                         'tvec__stop_words': [frozenset({'a', 'about', 'above',\n",
       "                                                         'across', 'after',\n",
       "                                                         'afterwards', 'again',\n",
       "                                                         'against', 'all',\n",
       "                                                         'almost', 'alone',\n",
       "                                                         'along', 'already',\n",
       "                                                         'also', 'although',\n",
       "                                                         'always', 'am',\n",
       "                                                         'among', 'amongst',\n",
       "                                                         'amoungst', 'amount',\n",
       "                                                         'an', 'and', 'another',\n",
       "                                                         'any', 'anyhow',\n",
       "                                                         'anyone', 'anything',\n",
       "                                                         'anyway', 'anywhere', ...})]})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipe_tvlogreg = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('logreg', LogisticRegression(max_iter=10000))\n",
    "])\n",
    "\n",
    "pipe_tvlogreg_params = {\n",
    "    'tvec__max_features': [2000, 3000, 4000],\n",
    "    'tvec__min_df': [2, 3],\n",
    "    'tvec__max_df': [0.9, 0.95],\n",
    "    'tvec__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'tvec__stop_words': [total_stop_words],\n",
    "    'logreg__C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "gs_tvlogreg = GridSearchCV(pipe_tvlogreg, \n",
    "                  param_grid=pipe_tvlogreg_params, \n",
    "                  cv=5,\n",
    "                 n_jobs=-1)\n",
    "\n",
    "gs_tvlogreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18bdcd5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tvec x logreg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Best Score</th>\n",
       "      <td>0.73354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Score (accuracy)</th>\n",
       "      <td>0.75632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.76259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.68387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.72109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Best Params</th>\n",
       "      <td>{'logreg__C': 1, 'tvec__max_df': 0.9, 'tvec__max_features': 4000, 'tvec__min_df': 2, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': ('myself', 'all', 'both', 'take', 'themselves', 'hundred', 'each', 'further', 'first', 'someone', 'hasnt', 'when', 'may', 'upon', 'by', 'system', 'enough', 'too', 'they', 'therefore', 'across', 'per', 'to', 'whence', 'within', 'twenty', 'part', 'will', 'whereupon', 'everything', 'name', 'bill', 'afterwards', 'who', 'mine', 'nothing', 'give', 'some', 'thereby', 'can', 'therein', 'thin', 'something', 'get', 'it', 'beforehand', 'etc', 'back', 'always', 'amongst', 'top', 'nowhere', 'thus', 'rather', 'until', 'anyhow', 'above', 'but', 'or', 'on', 'me', 'last', 'successes', 'i', 'bottom', 'problem', 'very', 'twelve', 'still', 'toward', 'ten', 'neither', 'mill', 'them', 'wherein', 'around', 'thereupon', 've', 'thick', 'a', 'serious', 'detail', 'least', 'anyone', 'over', 'put', 'anything', 'cant', 'else', 'had', 'throughout', 'he', 'beyond', 'us', 'were', 'how', 'under', 'eg', 'few', 'interest', ...)}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          tvec x logreg\n",
       "Best Score             0.73354                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "Test Score (accuracy)  0.75632                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "Precision              0.76259                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "Recall                 0.68387                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "F1                     0.72109                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "Best Params            {'logreg__C': 1, 'tvec__max_df': 0.9, 'tvec__max_features': 4000, 'tvec__min_df': 2, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': ('myself', 'all', 'both', 'take', 'themselves', 'hundred', 'each', 'further', 'first', 'someone', 'hasnt', 'when', 'may', 'upon', 'by', 'system', 'enough', 'too', 'they', 'therefore', 'across', 'per', 'to', 'whence', 'within', 'twenty', 'part', 'will', 'whereupon', 'everything', 'name', 'bill', 'afterwards', 'who', 'mine', 'nothing', 'give', 'some', 'thereby', 'can', 'therein', 'thin', 'something', 'get', 'it', 'beforehand', 'etc', 'back', 'always', 'amongst', 'top', 'nowhere', 'thus', 'rather', 'until', 'anyhow', 'above', 'but', 'or', 'on', 'me', 'last', 'successes', 'i', 'bottom', 'problem', 'very', 'twelve', 'still', 'toward', 'ten', 'neither', 'mill', 'them', 'wherein', 'around', 'thereupon', 've', 'thick', 'a', 'serious', 'detail', 'least', 'anyone', 'over', 'put', 'anything', 'cant', 'else', 'had', 'throughout', 'he', 'beyond', 'us', 'were', 'how', 'under', 'eg', 'few', 'interest', ...)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results(gs_tvlogreg, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f17fdd",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer x Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e243113e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.37 s, sys: 464 ms, total: 7.84 s\n",
      "Wall time: 4min 55s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                       ('rf', RandomForestClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'rf__min_samples_leaf': [1, 2, 3],\n",
       "                         'rf__n_estimators': [100, 150],\n",
       "                         'tvec__max_df': [0.9, 0.95],\n",
       "                         'tvec__max_features': [2000, 3000, 4000, 5000, 7000],\n",
       "                         'tvec__min_df': [2, 3],\n",
       "                         'tvec__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       "                         'tvec__stop_words': [frozenset({'a', 'about', 'above',\n",
       "                                                         'across', 'after',\n",
       "                                                         'afterwards', 'again',\n",
       "                                                         'against', 'all',\n",
       "                                                         'almost', 'alone',\n",
       "                                                         'along', 'already',\n",
       "                                                         'also', 'although',\n",
       "                                                         'always', 'am',\n",
       "                                                         'among', 'amongst',\n",
       "                                                         'amoungst', 'amount',\n",
       "                                                         'an', 'and', 'another',\n",
       "                                                         'any', 'anyhow',\n",
       "                                                         'anyone', 'anything',\n",
       "                                                         'anyway', 'anywhere', ...})]})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipe_tvrf = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "pipe_tvrf_params = {\n",
    "    'tvec__max_features': [2000, 3000, 4000, 5000, 7000],\n",
    "    'tvec__min_df': [2, 3],\n",
    "    'tvec__max_df': [.9, .95],\n",
    "    'tvec__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'tvec__stop_words': [total_stop_words],\n",
    "    'rf__n_estimators': [100, 150],\n",
    "    'rf__min_samples_leaf': [1,2,3]\n",
    "}\n",
    "\n",
    "\n",
    "gs_tvrf = GridSearchCV(pipe_tvrf, \n",
    "                  param_grid=pipe_tvrf_params,\n",
    "                  cv=5,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "gs_tvrf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e08d521f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tvec x rf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Best Score</th>\n",
       "      <td>0.71769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Score (accuracy)</th>\n",
       "      <td>0.75037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.7415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.70323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.72185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Best Params</th>\n",
       "      <td>{'rf__min_samples_leaf': 3, 'rf__n_estimators': 150, 'tvec__max_df': 0.95, 'tvec__max_features': 7000, 'tvec__min_df': 2, 'tvec__ngram_range': (1, 3), 'tvec__stop_words': ('myself', 'all', 'both', 'take', 'themselves', 'hundred', 'each', 'further', 'first', 'someone', 'hasnt', 'when', 'may', 'upon', 'by', 'system', 'enough', 'too', 'they', 'therefore', 'across', 'per', 'to', 'whence', 'within', 'twenty', 'part', 'will', 'whereupon', 'everything', 'name', 'bill', 'afterwards', 'who', 'mine', 'nothing', 'give', 'some', 'thereby', 'can', 'therein', 'thin', 'something', 'get', 'it', 'beforehand', 'etc', 'back', 'always', 'amongst', 'top', 'nowhere', 'thus', 'rather', 'until', 'anyhow', 'above', 'but', 'or', 'on', 'me', 'last', 'successes', 'i', 'bottom', 'problem', 'very', 'twelve', 'still', 'toward', 'ten', 'neither', 'mill', 'them', 'wherein', 'around', 'thereupon', 've', 'thick', 'a', 'serious', 'detail', 'least', 'anyone', 'over', 'put', 'anything', 'cant', 'else', 'had', 'throughout', 'he', 'beyond', 'us', 'were', 'how', 'under', 'eg', 'few', 'interest', ...)}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   tvec x rf\n",
       "Best Score             0.71769                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "Test Score (accuracy)  0.75037                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "Precision              0.7415                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "Recall                 0.70323                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "F1                     0.72185                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "Best Params            {'rf__min_samples_leaf': 3, 'rf__n_estimators': 150, 'tvec__max_df': 0.95, 'tvec__max_features': 7000, 'tvec__min_df': 2, 'tvec__ngram_range': (1, 3), 'tvec__stop_words': ('myself', 'all', 'both', 'take', 'themselves', 'hundred', 'each', 'further', 'first', 'someone', 'hasnt', 'when', 'may', 'upon', 'by', 'system', 'enough', 'too', 'they', 'therefore', 'across', 'per', 'to', 'whence', 'within', 'twenty', 'part', 'will', 'whereupon', 'everything', 'name', 'bill', 'afterwards', 'who', 'mine', 'nothing', 'give', 'some', 'thereby', 'can', 'therein', 'thin', 'something', 'get', 'it', 'beforehand', 'etc', 'back', 'always', 'amongst', 'top', 'nowhere', 'thus', 'rather', 'until', 'anyhow', 'above', 'but', 'or', 'on', 'me', 'last', 'successes', 'i', 'bottom', 'problem', 'very', 'twelve', 'still', 'toward', 'ten', 'neither', 'mill', 'them', 'wherein', 'around', 'thereupon', 've', 'thick', 'a', 'serious', 'detail', 'least', 'anyone', 'over', 'put', 'anything', 'cant', 'else', 'had', 'throughout', 'he', 'beyond', 'us', 'were', 'how', 'under', 'eg', 'few', 'interest', ...)}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results(gs_tvrf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d5401f",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer x Mulitnomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a942bde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.71 s, sys: 119 ms, total: 1.83 s\n",
      "Wall time: 27.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'nb__alpha': [1, 2, 3], 'tvec__max_df': [0.9, 0.95],\n",
       "                         'tvec__max_features': [2000, 3000, 4000, 5000],\n",
       "                         'tvec__min_df': [2, 3],\n",
       "                         'tvec__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       "                         'tvec__stop_words': [frozenset({'a', 'about', 'above',\n",
       "                                                         'across', 'after',\n",
       "                                                         'afterwards', 'again',\n",
       "                                                         'against', 'all',\n",
       "                                                         'almost', 'alone',\n",
       "                                                         'along', 'already',\n",
       "                                                         'also', 'although',\n",
       "                                                         'always', 'am',\n",
       "                                                         'among', 'amongst',\n",
       "                                                         'amoungst', 'amount',\n",
       "                                                         'an', 'and', 'another',\n",
       "                                                         'any', 'anyhow',\n",
       "                                                         'anyone', 'anything',\n",
       "                                                         'anyway', 'anywhere', ...})]})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipe_tvnb = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe_tvnb_params = {\n",
    "    'tvec__max_features': [2000, 3000, 4000, 5000],\n",
    "    'tvec__min_df': [2, 3],\n",
    "    'tvec__max_df': [.9, .95],\n",
    "    'tvec__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'tvec__stop_words': [total_stop_words],\n",
    "    'nb__alpha': [1,2,3]\n",
    "}\n",
    "\n",
    "\n",
    "gs_tvnb = GridSearchCV(pipe_tvnb, \n",
    "                  param_grid=pipe_tvnb_params,\n",
    "                  cv=5,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "gs_tvnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c48ef2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tvec x nb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Best Score</th>\n",
       "      <td>0.73106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Score (accuracy)</th>\n",
       "      <td>0.75186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.77395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.65161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.70753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Best Params</th>\n",
       "      <td>{'nb__alpha': 1, 'tvec__max_df': 0.9, 'tvec__max_features': 2000, 'tvec__min_df': 2, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': ('myself', 'all', 'both', 'take', 'themselves', 'hundred', 'each', 'further', 'first', 'someone', 'hasnt', 'when', 'may', 'upon', 'by', 'system', 'enough', 'too', 'they', 'therefore', 'across', 'per', 'to', 'whence', 'within', 'twenty', 'part', 'will', 'whereupon', 'everything', 'name', 'bill', 'afterwards', 'who', 'mine', 'nothing', 'give', 'some', 'thereby', 'can', 'therein', 'thin', 'something', 'get', 'it', 'beforehand', 'etc', 'back', 'always', 'amongst', 'top', 'nowhere', 'thus', 'rather', 'until', 'anyhow', 'above', 'but', 'or', 'on', 'me', 'last', 'successes', 'i', 'bottom', 'problem', 'very', 'twelve', 'still', 'toward', 'ten', 'neither', 'mill', 'them', 'wherein', 'around', 'thereupon', 've', 'thick', 'a', 'serious', 'detail', 'least', 'anyone', 'over', 'put', 'anything', 'cant', 'else', 'had', 'throughout', 'he', 'beyond', 'us', 'were', 'how', 'under', 'eg', 'few', 'interest', ...)}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              tvec x nb\n",
       "Best Score             0.73106                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "Test Score (accuracy)  0.75186                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "Precision              0.77395                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "Recall                 0.65161                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "F1                     0.70753                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "Best Params            {'nb__alpha': 1, 'tvec__max_df': 0.9, 'tvec__max_features': 2000, 'tvec__min_df': 2, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': ('myself', 'all', 'both', 'take', 'themselves', 'hundred', 'each', 'further', 'first', 'someone', 'hasnt', 'when', 'may', 'upon', 'by', 'system', 'enough', 'too', 'they', 'therefore', 'across', 'per', 'to', 'whence', 'within', 'twenty', 'part', 'will', 'whereupon', 'everything', 'name', 'bill', 'afterwards', 'who', 'mine', 'nothing', 'give', 'some', 'thereby', 'can', 'therein', 'thin', 'something', 'get', 'it', 'beforehand', 'etc', 'back', 'always', 'amongst', 'top', 'nowhere', 'thus', 'rather', 'until', 'anyhow', 'above', 'but', 'or', 'on', 'me', 'last', 'successes', 'i', 'bottom', 'problem', 'very', 'twelve', 'still', 'toward', 'ten', 'neither', 'mill', 'them', 'wherein', 'around', 'thereupon', 've', 'thick', 'a', 'serious', 'detail', 'least', 'anyone', 'over', 'put', 'anything', 'cant', 'else', 'had', 'throughout', 'he', 'beyond', 'us', 'were', 'how', 'under', 'eg', 'few', 'interest', ...)}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results(gs_tvnb, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34536d8",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aa6517",
   "metadata": {},
   "source": [
    "With lemmatization, the CountVectorizer x Multinomial Naive Bayes model performed the best (highest accuracy score at 0.77). It also has the highest F1 score at 0.75 (good balance of precision and recall). \n",
    "\n",
    "A point to note: all models are not too dissimilar in terms of accuracy score - they range from 0.73 to 0.77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27e2c0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_df = pd.concat([results(gs_cvlogreg, X_test, y_test), results(gs_cvrf, X_test, y_test), results(gs_cvnb, X_test, y_test), results(gs_tvlogreg, X_test, y_test), results(gs_tvrf, X_test, y_test), results(gs_tvnb, X_test, y_test)],axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e582d759",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best Score</th>\n",
       "      <th>Test Score (accuracy)</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cvec x nb</th>\n",
       "      <td>0.74096</td>\n",
       "      <td>0.77117</td>\n",
       "      <td>0.75161</td>\n",
       "      <td>0.75161</td>\n",
       "      <td>0.75161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cvec x rf</th>\n",
       "      <td>0.72214</td>\n",
       "      <td>0.75632</td>\n",
       "      <td>0.76449</td>\n",
       "      <td>0.68065</td>\n",
       "      <td>0.72014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tvec x logreg</th>\n",
       "      <td>0.73354</td>\n",
       "      <td>0.75632</td>\n",
       "      <td>0.76259</td>\n",
       "      <td>0.68387</td>\n",
       "      <td>0.72109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tvec x nb</th>\n",
       "      <td>0.73106</td>\n",
       "      <td>0.75186</td>\n",
       "      <td>0.77395</td>\n",
       "      <td>0.65161</td>\n",
       "      <td>0.70753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tvec x rf</th>\n",
       "      <td>0.71769</td>\n",
       "      <td>0.75037</td>\n",
       "      <td>0.7415</td>\n",
       "      <td>0.70323</td>\n",
       "      <td>0.72185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cvec x logreg</th>\n",
       "      <td>0.72463</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.72696</td>\n",
       "      <td>0.6871</td>\n",
       "      <td>0.70647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Best Score Test Score (accuracy) Precision   Recall       F1\n",
       "cvec x nb      0.74096    0.77117               0.75161   0.75161  0.75161\n",
       "cvec x rf      0.72214    0.75632               0.76449   0.68065  0.72014\n",
       "tvec x logreg  0.73354    0.75632               0.76259   0.68387  0.72109\n",
       "tvec x nb      0.73106    0.75186               0.77395   0.65161  0.70753\n",
       "tvec x rf      0.71769    0.75037               0.7415    0.70323  0.72185\n",
       "cvec x logreg  0.72463    0.737                 0.72696   0.6871   0.70647"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_df.sort_values(ascending=False, by='Test Score (accuracy)').drop(columns='Best Params')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd87c9c2",
   "metadata": {},
   "source": [
    "## Part 3: Without Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8c14e4",
   "metadata": {},
   "source": [
    "The rationale for removing lemmatization in the steps below is to see if we could make our modelling 'simpler' by avoiding excessive tokenization of the corpus. I only performed these on the Multinomial Naive Bayes and Random Forest models as they have the highest accuracy scores in Part 1 and Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0d79d8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_non_lemma = problems_success['content']\n",
    "y_non_lemma = problems_success['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "34c69975",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nl_train, X_nl_test, y_nl_train, y_nl_test = train_test_split(X_non_lemma, y_non_lemma, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eeb493",
   "metadata": {},
   "source": [
    "### Count Vectorizer x Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c2a5fa00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.78 s, sys: 279 ms, total: 3.06 s\n",
      "Wall time: 35 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'cvec__max_df': [0.9, 0.95],\n",
       "                         'cvec__max_features': [2000, 3000, 4000, 5000, 7500],\n",
       "                         'cvec__min_df': [2, 3],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       "                         'cvec__stop_words': [frozenset({'a', 'about', 'above',\n",
       "                                                         'across', 'after',\n",
       "                                                         'afterwards', 'again',\n",
       "                                                         'against', 'all',\n",
       "                                                         'almost', 'alone',\n",
       "                                                         'along', 'already',\n",
       "                                                         'also', 'although',\n",
       "                                                         'always', 'am',\n",
       "                                                         'among', 'amongst',\n",
       "                                                         'amoungst', 'amount',\n",
       "                                                         'an', 'and', 'another',\n",
       "                                                         'any', 'anyhow',\n",
       "                                                         'anyone', 'anything',\n",
       "                                                         'anyway', 'anywhere', ...})],\n",
       "                         'nb__alpha': [1, 2, 3]})"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipe_nl_cvnb = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe_nl_cvnb_params = {\n",
    "    'cvec__max_features': [2000, 3000, 4000, 5000,7500],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [0.9, 0.95],\n",
    "    'cvec__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'cvec__stop_words': [total_stop_words],    \n",
    "    'nb__alpha': [1,2,3]\n",
    "}\n",
    "\n",
    "gs_nl_cvnb = GridSearchCV(pipe_nl_cvnb, \n",
    "                  param_grid=pipe_nl_cvnb_params, \n",
    "                  cv=5,\n",
    "                 n_jobs=-1)\n",
    "\n",
    "gs_nl_cvnb.fit(X_nl_train, y_nl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b5208090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cvec x nb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Best Score</th>\n",
       "      <td>0.73799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Score (accuracy)</th>\n",
       "      <td>0.76077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.74587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.72903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.73736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Best Params</th>\n",
       "      <td>{'cvec__max_df': 0.9, 'cvec__max_features': 2000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ('myself', 'all', 'both', 'take', 'themselves', 'hundred', 'each', 'further', 'first', 'someone', 'hasnt', 'when', 'may', 'upon', 'by', 'system', 'enough', 'too', 'they', 'therefore', 'across', 'per', 'to', 'whence', 'within', 'twenty', 'part', 'will', 'whereupon', 'everything', 'name', 'bill', 'afterwards', 'who', 'mine', 'nothing', 'give', 'some', 'thereby', 'can', 'therein', 'thin', 'something', 'get', 'it', 'beforehand', 'etc', 'back', 'always', 'amongst', 'top', 'nowhere', 'thus', 'rather', 'until', 'anyhow', 'above', 'but', 'or', 'on', 'me', 'last', 'successes', 'i', 'bottom', 'problem', 'very', 'twelve', 'still', 'toward', 'ten', 'neither', 'mill', 'them', 'wherein', 'around', 'thereupon', 've', 'thick', 'a', 'serious', 'detail', 'least', 'anyone', 'over', 'put', 'anything', 'cant', 'else', 'had', 'throughout', 'he', 'beyond', 'us', 'were', 'how', 'under', 'eg', 'few', 'interest', ...), 'nb__alpha': 2}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              cvec x nb\n",
       "Best Score             0.73799                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "Test Score (accuracy)  0.76077                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "Precision              0.74587                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "Recall                 0.72903                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "F1                     0.73736                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "Best Params            {'cvec__max_df': 0.9, 'cvec__max_features': 2000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ('myself', 'all', 'both', 'take', 'themselves', 'hundred', 'each', 'further', 'first', 'someone', 'hasnt', 'when', 'may', 'upon', 'by', 'system', 'enough', 'too', 'they', 'therefore', 'across', 'per', 'to', 'whence', 'within', 'twenty', 'part', 'will', 'whereupon', 'everything', 'name', 'bill', 'afterwards', 'who', 'mine', 'nothing', 'give', 'some', 'thereby', 'can', 'therein', 'thin', 'something', 'get', 'it', 'beforehand', 'etc', 'back', 'always', 'amongst', 'top', 'nowhere', 'thus', 'rather', 'until', 'anyhow', 'above', 'but', 'or', 'on', 'me', 'last', 'successes', 'i', 'bottom', 'problem', 'very', 'twelve', 'still', 'toward', 'ten', 'neither', 'mill', 'them', 'wherein', 'around', 'thereupon', 've', 'thick', 'a', 'serious', 'detail', 'least', 'anyone', 'over', 'put', 'anything', 'cant', 'else', 'had', 'throughout', 'he', 'beyond', 'us', 'were', 'how', 'under', 'eg', 'few', 'interest', ...), 'nb__alpha': 2}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results(gs_nl_cvnb, X_nl_test, y_nl_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f124fbca",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer x Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f002fc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.96 s, sys: 119 ms, total: 2.08 s\n",
      "Wall time: 33.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'nb__alpha': [1, 2, 3], 'tvec__max_df': [0.9, 0.95],\n",
       "                         'tvec__max_features': [2000, 3000, 4000, 5000, 7500],\n",
       "                         'tvec__min_df': [2, 3],\n",
       "                         'tvec__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       "                         'tvec__stop_words': [frozenset({'a', 'about', 'above',\n",
       "                                                         'across', 'after',\n",
       "                                                         'afterwards', 'again',\n",
       "                                                         'against', 'all',\n",
       "                                                         'almost', 'alone',\n",
       "                                                         'along', 'already',\n",
       "                                                         'also', 'although',\n",
       "                                                         'always', 'am',\n",
       "                                                         'among', 'amongst',\n",
       "                                                         'amoungst', 'amount',\n",
       "                                                         'an', 'and', 'another',\n",
       "                                                         'any', 'anyhow',\n",
       "                                                         'anyone', 'anything',\n",
       "                                                         'anyway', 'anywhere', ...})]})"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipe_nl_tvnb = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe_nl_tvnb_params = {\n",
    "    'tvec__max_features': [2000, 3000, 4000, 5000,7500],\n",
    "    'tvec__min_df': [2, 3],\n",
    "    'tvec__max_df': [0.9, 0.95],\n",
    "    'tvec__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'tvec__stop_words': [total_stop_words],\n",
    "    'nb__alpha': [1,2,3]\n",
    "}\n",
    "\n",
    "gs_nl_tvnb = GridSearchCV(pipe_nl_tvnb, \n",
    "                  param_grid=pipe_nl_tvnb_params, \n",
    "                  cv=5,\n",
    "                 n_jobs=-1)\n",
    "\n",
    "gs_nl_tvnb.fit(X_nl_train, y_nl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0a10d0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tvec x nb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Best Score</th>\n",
       "      <td>0.72562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Score (accuracy)</th>\n",
       "      <td>0.75929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.79839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.63871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.70968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Best Params</th>\n",
       "      <td>{'nb__alpha': 1, 'tvec__max_df': 0.9, 'tvec__max_features': 3000, 'tvec__min_df': 2, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': ('myself', 'all', 'both', 'take', 'themselves', 'hundred', 'each', 'further', 'first', 'someone', 'hasnt', 'when', 'may', 'upon', 'by', 'system', 'enough', 'too', 'they', 'therefore', 'across', 'per', 'to', 'whence', 'within', 'twenty', 'part', 'will', 'whereupon', 'everything', 'name', 'bill', 'afterwards', 'who', 'mine', 'nothing', 'give', 'some', 'thereby', 'can', 'therein', 'thin', 'something', 'get', 'it', 'beforehand', 'etc', 'back', 'always', 'amongst', 'top', 'nowhere', 'thus', 'rather', 'until', 'anyhow', 'above', 'but', 'or', 'on', 'me', 'last', 'successes', 'i', 'bottom', 'problem', 'very', 'twelve', 'still', 'toward', 'ten', 'neither', 'mill', 'them', 'wherein', 'around', 'thereupon', 've', 'thick', 'a', 'serious', 'detail', 'least', 'anyone', 'over', 'put', 'anything', 'cant', 'else', 'had', 'throughout', 'he', 'beyond', 'us', 'were', 'how', 'under', 'eg', 'few', 'interest', ...)}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              tvec x nb\n",
       "Best Score             0.72562                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "Test Score (accuracy)  0.75929                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "Precision              0.79839                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "Recall                 0.63871                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "F1                     0.70968                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "Best Params            {'nb__alpha': 1, 'tvec__max_df': 0.9, 'tvec__max_features': 3000, 'tvec__min_df': 2, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': ('myself', 'all', 'both', 'take', 'themselves', 'hundred', 'each', 'further', 'first', 'someone', 'hasnt', 'when', 'may', 'upon', 'by', 'system', 'enough', 'too', 'they', 'therefore', 'across', 'per', 'to', 'whence', 'within', 'twenty', 'part', 'will', 'whereupon', 'everything', 'name', 'bill', 'afterwards', 'who', 'mine', 'nothing', 'give', 'some', 'thereby', 'can', 'therein', 'thin', 'something', 'get', 'it', 'beforehand', 'etc', 'back', 'always', 'amongst', 'top', 'nowhere', 'thus', 'rather', 'until', 'anyhow', 'above', 'but', 'or', 'on', 'me', 'last', 'successes', 'i', 'bottom', 'problem', 'very', 'twelve', 'still', 'toward', 'ten', 'neither', 'mill', 'them', 'wherein', 'around', 'thereupon', 've', 'thick', 'a', 'serious', 'detail', 'least', 'anyone', 'over', 'put', 'anything', 'cant', 'else', 'had', 'throughout', 'he', 'beyond', 'us', 'were', 'how', 'under', 'eg', 'few', 'interest', ...)}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results(gs_nl_tvnb, X_nl_test, y_nl_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93f15a5",
   "metadata": {},
   "source": [
    "### Count Vectorizer x Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fd1c3c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.8 s, sys: 461 ms, total: 7.26 s\n",
      "Wall time: 4min 34s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('rf', RandomForestClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'cvec__max_df': [0.9, 0.95],\n",
       "                         'cvec__max_features': [2000, 3000, 4000, 5000, 7000],\n",
       "                         'cvec__min_df': [2, 3],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       "                         'cvec__stop_words': [frozenset({'a', 'about', 'above',\n",
       "                                                         'across', 'after',\n",
       "                                                         'afterwards', 'again',\n",
       "                                                         'against', 'all',\n",
       "                                                         'almost', 'alone',\n",
       "                                                         'along', 'already',\n",
       "                                                         'also', 'although',\n",
       "                                                         'always', 'am',\n",
       "                                                         'among', 'amongst',\n",
       "                                                         'amoungst', 'amount',\n",
       "                                                         'an', 'and', 'another',\n",
       "                                                         'any', 'anyhow',\n",
       "                                                         'anyone', 'anything',\n",
       "                                                         'anyway', 'anywhere', ...})],\n",
       "                         'rf__min_samples_leaf': [1, 2, 3],\n",
       "                         'rf__n_estimators': [100, 150]})"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipe_nl_cvrf = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "pipe_nl_cvrf_params = {\n",
    "    'cvec__max_features': [2000, 3000, 4000, 5000, 7000],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [0.9, 0.95],\n",
    "    'cvec__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'cvec__stop_words': [total_stop_words],\n",
    "    'rf__n_estimators': [100, 150],\n",
    "    'rf__min_samples_leaf': [1,2,3]\n",
    "}\n",
    "\n",
    "gs_nl_cvrf = GridSearchCV(pipe_nl_cvrf, \n",
    "                  param_grid=pipe_nl_cvrf_params, \n",
    "                  cv=5,\n",
    "                 n_jobs=-1)\n",
    "\n",
    "gs_nl_cvrf.fit(X_nl_train, y_nl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "029529d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cvec x rf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Best Score</th>\n",
       "      <td>0.72413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Score (accuracy)</th>\n",
       "      <td>0.74591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.76834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.64194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.69947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Best Params</th>\n",
       "      <td>{'cvec__max_df': 0.9, 'cvec__max_features': 7000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': ('myself', 'all', 'both', 'take', 'themselves', 'hundred', 'each', 'further', 'first', 'someone', 'hasnt', 'when', 'may', 'upon', 'by', 'system', 'enough', 'too', 'they', 'therefore', 'across', 'per', 'to', 'whence', 'within', 'twenty', 'part', 'will', 'whereupon', 'everything', 'name', 'bill', 'afterwards', 'who', 'mine', 'nothing', 'give', 'some', 'thereby', 'can', 'therein', 'thin', 'something', 'get', 'it', 'beforehand', 'etc', 'back', 'always', 'amongst', 'top', 'nowhere', 'thus', 'rather', 'until', 'anyhow', 'above', 'but', 'or', 'on', 'me', 'last', 'successes', 'i', 'bottom', 'problem', 'very', 'twelve', 'still', 'toward', 'ten', 'neither', 'mill', 'them', 'wherein', 'around', 'thereupon', 've', 'thick', 'a', 'serious', 'detail', 'least', 'anyone', 'over', 'put', 'anything', 'cant', 'else', 'had', 'throughout', 'he', 'beyond', 'us', 'were', 'how', 'under', 'eg', 'few', 'interest', ...), 'rf__min_samples_leaf': 3, 'rf__n_estimators': 100}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  cvec x rf\n",
       "Best Score             0.72413                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "Test Score (accuracy)  0.74591                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "Precision              0.76834                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "Recall                 0.64194                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "F1                     0.69947                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "Best Params            {'cvec__max_df': 0.9, 'cvec__max_features': 7000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': ('myself', 'all', 'both', 'take', 'themselves', 'hundred', 'each', 'further', 'first', 'someone', 'hasnt', 'when', 'may', 'upon', 'by', 'system', 'enough', 'too', 'they', 'therefore', 'across', 'per', 'to', 'whence', 'within', 'twenty', 'part', 'will', 'whereupon', 'everything', 'name', 'bill', 'afterwards', 'who', 'mine', 'nothing', 'give', 'some', 'thereby', 'can', 'therein', 'thin', 'something', 'get', 'it', 'beforehand', 'etc', 'back', 'always', 'amongst', 'top', 'nowhere', 'thus', 'rather', 'until', 'anyhow', 'above', 'but', 'or', 'on', 'me', 'last', 'successes', 'i', 'bottom', 'problem', 'very', 'twelve', 'still', 'toward', 'ten', 'neither', 'mill', 'them', 'wherein', 'around', 'thereupon', 've', 'thick', 'a', 'serious', 'detail', 'least', 'anyone', 'over', 'put', 'anything', 'cant', 'else', 'had', 'throughout', 'he', 'beyond', 'us', 'were', 'how', 'under', 'eg', 'few', 'interest', ...), 'rf__min_samples_leaf': 3, 'rf__n_estimators': 100}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results(gs_nl_cvrf, X_nl_test, y_nl_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3255190",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer x Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "aae5ee53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.31 s, sys: 992 ms, total: 8.3 s\n",
      "Wall time: 4min 58s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tvec', CountVectorizer()),\n",
       "                                       ('rf', RandomForestClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'rf__min_samples_leaf': [1, 2, 3],\n",
       "                         'rf__n_estimators': [100, 150],\n",
       "                         'tvec__max_df': [0.9, 0.95],\n",
       "                         'tvec__max_features': [2000, 3000, 4000, 5000, 7000],\n",
       "                         'tvec__min_df': [2, 3],\n",
       "                         'tvec__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       "                         'tvec__stop_words': [frozenset({'a', 'about', 'above',\n",
       "                                                         'across', 'after',\n",
       "                                                         'afterwards', 'again',\n",
       "                                                         'against', 'all',\n",
       "                                                         'almost', 'alone',\n",
       "                                                         'along', 'already',\n",
       "                                                         'also', 'although',\n",
       "                                                         'always', 'am',\n",
       "                                                         'among', 'amongst',\n",
       "                                                         'amoungst', 'amount',\n",
       "                                                         'an', 'and', 'another',\n",
       "                                                         'any', 'anyhow',\n",
       "                                                         'anyone', 'anything',\n",
       "                                                         'anyway', 'anywhere', ...})]})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipe_nl_tvrf = Pipeline([\n",
    "    ('tvec', CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "pipe_nl_tvrf_params = {\n",
    "    'tvec__max_features': [2000, 3000, 4000, 5000, 7000],\n",
    "    'tvec__min_df': [2, 3],\n",
    "    'tvec__max_df': [0.9, 0.95],\n",
    "    'tvec__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'tvec__stop_words': [total_stop_words],   \n",
    "    'rf__n_estimators': [100, 150],\n",
    "    'rf__min_samples_leaf': [1,2,3]\n",
    "}\n",
    "\n",
    "gs_nl_tvrf = GridSearchCV(pipe_nl_tvrf, \n",
    "                  param_grid=pipe_nl_tvrf_params, \n",
    "                  cv=5,\n",
    "                 n_jobs=-1)\n",
    "\n",
    "gs_nl_tvrf.fit(X_nl_train, y_nl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9311f50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tvec x rf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Best Score</th>\n",
       "      <td>0.72562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Score (accuracy)</th>\n",
       "      <td>0.75186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.78039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.64194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.70442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Best Params</th>\n",
       "      <td>{'rf__min_samples_leaf': 3, 'rf__n_estimators': 100, 'tvec__max_df': 0.95, 'tvec__max_features': 4000, 'tvec__min_df': 2, 'tvec__ngram_range': (1, 3), 'tvec__stop_words': ('myself', 'all', 'both', 'take', 'themselves', 'hundred', 'each', 'further', 'first', 'someone', 'hasnt', 'when', 'may', 'upon', 'by', 'system', 'enough', 'too', 'they', 'therefore', 'across', 'per', 'to', 'whence', 'within', 'twenty', 'part', 'will', 'whereupon', 'everything', 'name', 'bill', 'afterwards', 'who', 'mine', 'nothing', 'give', 'some', 'thereby', 'can', 'therein', 'thin', 'something', 'get', 'it', 'beforehand', 'etc', 'back', 'always', 'amongst', 'top', 'nowhere', 'thus', 'rather', 'until', 'anyhow', 'above', 'but', 'or', 'on', 'me', 'last', 'successes', 'i', 'bottom', 'problem', 'very', 'twelve', 'still', 'toward', 'ten', 'neither', 'mill', 'them', 'wherein', 'around', 'thereupon', 've', 'thick', 'a', 'serious', 'detail', 'least', 'anyone', 'over', 'put', 'anything', 'cant', 'else', 'had', 'throughout', 'he', 'beyond', 'us', 'were', 'how', 'under', 'eg', 'few', 'interest', ...)}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   tvec x rf\n",
       "Best Score             0.72562                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "Test Score (accuracy)  0.75186                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "Precision              0.78039                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "Recall                 0.64194                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "F1                     0.70442                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "Best Params            {'rf__min_samples_leaf': 3, 'rf__n_estimators': 100, 'tvec__max_df': 0.95, 'tvec__max_features': 4000, 'tvec__min_df': 2, 'tvec__ngram_range': (1, 3), 'tvec__stop_words': ('myself', 'all', 'both', 'take', 'themselves', 'hundred', 'each', 'further', 'first', 'someone', 'hasnt', 'when', 'may', 'upon', 'by', 'system', 'enough', 'too', 'they', 'therefore', 'across', 'per', 'to', 'whence', 'within', 'twenty', 'part', 'will', 'whereupon', 'everything', 'name', 'bill', 'afterwards', 'who', 'mine', 'nothing', 'give', 'some', 'thereby', 'can', 'therein', 'thin', 'something', 'get', 'it', 'beforehand', 'etc', 'back', 'always', 'amongst', 'top', 'nowhere', 'thus', 'rather', 'until', 'anyhow', 'above', 'but', 'or', 'on', 'me', 'last', 'successes', 'i', 'bottom', 'problem', 'very', 'twelve', 'still', 'toward', 'ten', 'neither', 'mill', 'them', 'wherein', 'around', 'thereupon', 've', 'thick', 'a', 'serious', 'detail', 'least', 'anyone', 'over', 'put', 'anything', 'cant', 'else', 'had', 'throughout', 'he', 'beyond', 'us', 'were', 'how', 'under', 'eg', 'few', 'interest', ...)}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results(gs_nl_tvrf, X_nl_test, y_nl_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada92760",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92247cb0",
   "metadata": {},
   "source": [
    "By removing the lemmatization step, our models did not perform better. They are fairly similar for the Random Forest models. With this in mind, a model with lemmatization performed in the pre-processing step will be chosen as the final model to deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7171a1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_nl_df = pd.concat([results(gs_nl_tvrf, X_nl_test, y_nl_test), results(gs_nl_cvrf, X_nl_test, y_nl_test), results(gs_nl_cvnb, X_nl_test, y_nl_test), results(gs_nl_tvnb, X_nl_test, y_nl_test)],axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e10d59e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best Score</th>\n",
       "      <th>Test Score (accuracy)</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cvec x nb</th>\n",
       "      <td>0.73799</td>\n",
       "      <td>0.76077</td>\n",
       "      <td>0.74587</td>\n",
       "      <td>0.72903</td>\n",
       "      <td>0.73736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tvec x nb</th>\n",
       "      <td>0.72562</td>\n",
       "      <td>0.75929</td>\n",
       "      <td>0.79839</td>\n",
       "      <td>0.63871</td>\n",
       "      <td>0.70968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tvec x rf</th>\n",
       "      <td>0.72562</td>\n",
       "      <td>0.75186</td>\n",
       "      <td>0.78039</td>\n",
       "      <td>0.64194</td>\n",
       "      <td>0.70442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cvec x rf</th>\n",
       "      <td>0.72413</td>\n",
       "      <td>0.74591</td>\n",
       "      <td>0.76834</td>\n",
       "      <td>0.64194</td>\n",
       "      <td>0.69947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Best Score Test Score (accuracy) Precision   Recall       F1\n",
       "cvec x nb  0.73799    0.76077               0.74587   0.72903  0.73736\n",
       "tvec x nb  0.72562    0.75929               0.79839   0.63871  0.70968\n",
       "tvec x rf  0.72562    0.75186               0.78039   0.64194  0.70442\n",
       "cvec x rf  0.72413    0.74591               0.76834   0.64194  0.69947"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_nl_df.drop(columns='Best Params').sort_values(ascending=False, by='Test Score (accuracy)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7645fd",
   "metadata": {},
   "source": [
    "## Final Model : Lemmatized & Count Vectorizer x Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b07477b",
   "metadata": {},
   "source": [
    "As mentioned previously, I have chosen the **Lemmatized & Count Vectorizer x Multinomial Naive Bayes** model. From the previous section, the model has obtained an accuracy score of **0.77**. \n",
    "\n",
    "I suspect that I can further improve the model by including punction marks such as '!'. CountVectorizer automatically drops punctuations but I suspect that they might be useful in classifcation here because. For example, '!' is found in 454 entries of 'British Success' versus 186 entries of 'British Problems' (not surprising since we often use '!' with something positive). It would be useful to find out if they play a part in improving the model.\n",
    "\n",
    "By doing so, I have improved the accuracy score slightly to **0.79**. The F1 score has also improved slightly (0.775)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "04ab0bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.2 s, sys: 96.7 ms, total: 1.3 s\n",
      "Wall time: 22.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'cvec__max_df': [0.9, 0.95],\n",
       "                         'cvec__max_features': [2000, 3000, 4000],\n",
       "                         'cvec__min_df': [2, 3],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       "                         'cvec__stop_words': [frozenset({'a', 'about', 'above',\n",
       "                                                         'across', 'after',\n",
       "                                                         'afterwards', 'again',\n",
       "                                                         'against', 'all',\n",
       "                                                         'almost', 'alone',\n",
       "                                                         'along', 'already',\n",
       "                                                         'also', 'although',\n",
       "                                                         'always', 'am',\n",
       "                                                         'among', 'amongst',\n",
       "                                                         'amoungst', 'amount',\n",
       "                                                         'an', 'and', 'another',\n",
       "                                                         'any', 'anyhow',\n",
       "                                                         'anyone', 'anything',\n",
       "                                                         'anyway', 'anywhere', ...})],\n",
       "                         'cvec__token_pattern': ['(?u)\\\\b\\\\w\\\\w+\\\\b|!|\\\\?|\\\\\"|\\\\\\''],\n",
       "                         'nb__alpha': [1, 2, 3]})"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipe_cvnb = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe_cvnb_params = {\n",
    "    'cvec__max_features': [2000, 3000, 4000],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [.9, .95],\n",
    "    'cvec__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'cvec__stop_words': [total_stop_words],\n",
    "    'cvec__token_pattern': [r\"(?u)\\b\\w\\w+\\b|!|\\?|\\\"|\\'\"],\n",
    "    'nb__alpha': [1,2,3]\n",
    "}\n",
    "\n",
    "\n",
    "gs_cvnb = GridSearchCV(pipe_cvnb, \n",
    "                  param_grid=pipe_cvnb_params,\n",
    "                  cv=5,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "gs_cvnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "33264575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7919762258543833"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_cvnb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0921dcc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cvec x nb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Best Score</th>\n",
       "      <td>0.74939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Score (accuracy)</th>\n",
       "      <td>0.79198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.7707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.78065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.77564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Best Params</th>\n",
       "      <td>{'cvec__max_df': 0.9, 'cvec__max_features': 4000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ('myself', 'all', 'both', 'take', 'themselves', 'hundred', 'each', 'further', 'first', 'someone', 'hasnt', 'when', 'may', 'upon', 'by', 'system', 'enough', 'too', 'they', 'therefore', 'across', 'per', 'to', 'whence', 'within', 'twenty', 'part', 'will', 'whereupon', 'everything', 'name', 'bill', 'afterwards', 'who', 'mine', 'nothing', 'give', 'some', 'thereby', 'can', 'therein', 'thin', 'something', 'get', 'it', 'beforehand', 'etc', 'back', 'always', 'amongst', 'top', 'nowhere', 'thus', 'rather', 'until', 'anyhow', 'above', 'but', 'or', 'on', 'me', 'last', 'successes', 'i', 'bottom', 'problem', 'very', 'twelve', 'still', 'toward', 'ten', 'neither', 'mill', 'them', 'wherein', 'around', 'thereupon', 've', 'thick', 'a', 'serious', 'detail', 'least', 'anyone', 'over', 'put', 'anything', 'cant', 'else', 'had', 'throughout', 'he', 'beyond', 'us', 'were', 'how', 'under', 'eg', 'few', 'interest', ...), 'cvec__token_pattern': '(?u)\\b\\w\\w+\\b|!|\\?|\\\"|\\'', 'nb__alpha': 2}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 cvec x nb\n",
       "Best Score             0.74939                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "Test Score (accuracy)  0.79198                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "Precision              0.7707                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "Recall                 0.78065                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "F1                     0.77564                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "Best Params            {'cvec__max_df': 0.9, 'cvec__max_features': 4000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ('myself', 'all', 'both', 'take', 'themselves', 'hundred', 'each', 'further', 'first', 'someone', 'hasnt', 'when', 'may', 'upon', 'by', 'system', 'enough', 'too', 'they', 'therefore', 'across', 'per', 'to', 'whence', 'within', 'twenty', 'part', 'will', 'whereupon', 'everything', 'name', 'bill', 'afterwards', 'who', 'mine', 'nothing', 'give', 'some', 'thereby', 'can', 'therein', 'thin', 'something', 'get', 'it', 'beforehand', 'etc', 'back', 'always', 'amongst', 'top', 'nowhere', 'thus', 'rather', 'until', 'anyhow', 'above', 'but', 'or', 'on', 'me', 'last', 'successes', 'i', 'bottom', 'problem', 'very', 'twelve', 'still', 'toward', 'ten', 'neither', 'mill', 'them', 'wherein', 'around', 'thereupon', 've', 'thick', 'a', 'serious', 'detail', 'least', 'anyone', 'over', 'put', 'anything', 'cant', 'else', 'had', 'throughout', 'he', 'beyond', 'us', 'were', 'how', 'under', 'eg', 'few', 'interest', ...), 'cvec__token_pattern': '(?u)\\b\\w\\w+\\b|!|\\?|\\\"|\\'', 'nb__alpha': 2}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results(gs_cvnb, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43efc2ef",
   "metadata": {},
   "source": [
    "### Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6923161",
   "metadata": {},
   "source": [
    "To evaluate the model, I start with understanding the features that help explain the difference between the two classes (i.e. the two sub-reddits). I used log probability difference as the metric here as it identifies words that are more frequent in one sub-reddit relative to the other. The results of each sub-reddit are reflected in the barplots below.\n",
    "\n",
    "**Key Observations**\n",
    "- Crane is the most 'different' word that explains why a post will be classified in 'British Problems' than in 'British Succes'. \n",
    "    - Crane probably means 'crane flies' a.k.a 'daddy long legs' which was one of the most common trigrams in our EDA section. These trigrams did not appear here because the optimised model I used deployed unigram as the ideal vectorization parameter.\n",
    "    - The reason why crane flies/daddy long legs appeared so frequently is probably due to the exceptionally wet summer that UK just had and people are complaining about these flies! [Source: Is UK seeing a record daddy long legs invasion? (Guardian)](https://www.theguardian.com/environment/2021/sep/22/is-the-uk-really-seeing-a-record-daddy-long-legs-invasion)\n",
    "- Words with negative connotation also top the list for 'British Problems': 'unable', 'worse', 'arse', 'fucking', 'upset'\n",
    "- For 'British Success', words associated with covid topped the list: 'vaccine', 'mask', 'nhs'. This is likely due to the full unwound of covid restrictions in the UK in July. \n",
    "- Words associated with 'overcoming' a problem/challenge have also appeared in the 'British Success' list: 'finally', 'managed'. \n",
    "- The words 'wife' and 'son' in 'British Success' make a surprising entrance - would be interesting to find out why!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3fb6f3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_diff = list(zip(words_cvnb, log_prob_diff_cvnb))\n",
    "log_diff_df = pd.DataFrame(log_diff, columns=['Word','Log Diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b5253443",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_cvnb = gs_cvnb.best_estimator_.steps[0][1].get_feature_names()\n",
    "log_prob_0_cvnb = gs_cvnb.best_estimator_.steps[1][1].feature_log_prob_[0]\n",
    "log_prob_1_cvnb = gs_cvnb.best_estimator_.steps[1][1].feature_log_prob_[1]\n",
    "log_prob_diff_cvnb = log_prob_1_cvnb - log_prob_0_cvnb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "28c72e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABDIAAAHQCAYAAABTHUoOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABmPklEQVR4nO3deZhlVXm//fsLTUSGBltag0TodkRBaaQgQQFRia8zEAckOCBGgsYQTdCoJGo0qKjRaIwDGmcc4gASjIqiDIIMBTTNIGq02yjw0yYNKIPI8Lx/7FVwKKq6q6ur6tSpuj/XVVfts/b0rN1VfZ569trrpKqQJEmSJEkaBBv1OwBJkiRJkqSJspAhSZIkSZIGhoUMSZIkSZI0MCxkSJIkSZKkgWEhQ5IkSZIkDQwLGZIkSZIkaWBYyJCmSZJK8pBx1h2a5PszHZPuLsllSfbtdxwzLclpSf6i33FIkqS7JLkhyYP6HcdMS7IqyX79jkODxUKG1KP9R3pzeyP5VZJPJNmi33HNhCSfbMWXZ41q/9fWfugGHn+tfzwnWdLOc0PP9T85yZ9uyHnXpqp2qqrTpuv4IybQ928leW3P6+3atRir7Q+nO15J0uyXZK8kZye5PsmaJGcl2b3fcU2V9t5ZSXYZ1X5ia993A4+/1j+ek+yb5I6evOSXSf5zOq9xVW1RVT+bruOPmEDff5TkeT2vH9eu+ei2G5IsmO54pbFYyJDu6ZlVtQXwGGB34B9GbzCH/9P+MfDikRetn88FfjqDMWzdrv8uwLeBEza0iDIAzgAe3/N6H+CKMdp+UlX/b6IHTcf/5yVpjkmyEDgZ+DdgEbAd8E/ALf2Maxr8GHjRyIsk9wX+BFg9Q+e/quUkW7bzXgGcmeRJM3T+fploXnJ2Vd020YPO4fxZfWCCK42jqq4EvgHsDHc+KvJXSX4C/KS1vSzJ/7Q7ISclecCowzwtyc+SXJPkXeP9UZlkxyTfbscZXQX/ZJIPJvlGq3yfleQP20iJa5NckWTXnu3/PsmVSX7bjrU+b7b/BTwuyX3a66cAK4A7/3hOslGSf0jy8yS/TvLpJFu1dZsm+WyS/0tyXZLzk9w/yTHA3sAHWh8+sK5Aqur/VdX7gDcDx45cuySPaHdprkv3aMidI0gmca3uvCOR5M3tTsun27W7LMlQz7avS/LTtu7yJAf2rDs0yfeTvLudZ2WSp7Z1E+n7Ge26j/x87A38KzA0qu2MdszHtmt7ffv+2J5YTktyTJKzgJuAByX509b369v507P9Q5Kc3tZdk+SL6/q3kST13cMAqurzVXV7Vd1cVadU1Qq48z3tsyMb565Rjwva60XpRp1e1d63TuzZdv8ky5P8pr3vPaW1b5XkP5Jc3fKMf06ycVs35ntJOu9t+cL1SVYk2Xk9+nk8cNDIeYCDgROA3/fEe6/2Pn9V+/rXJPdq67ZJN7rzunQ51pktj/kMsD3wX+29+bWjT9yrOr+sqjcCHwOO7Tn/ut6T/zndyJkbkvxXkvsmOb5d3/OTLOnZ/s7HkltO8+9Jvt5yj3OTPLhn2/cl+UU7zgVJ9u5ZN25OM8G+n0FXqBixd+vz6LaRvORZ7RzXtT4/oieWVely0xXAjUkWJHlhujzy/5Ic3XviJHskGW79+lWS96zt30bzl4UMaRxJHgg8Dbiop/kA4I+BRyZ5IvB24HnAtsDPgS+MOsyBwBDd6I79gcPGOM/mdCMPPgfcj+5N+oNJdurZ7Hl0I0O2obvb8gPgwvb6y8B72rEeDrwS2L2qtgT+P2BVW7dXkuvW0e3fAScBz2+vXwR8etQ2h7avJwAPArYARv44fzGwFfBA4L7AEcDNVXU0cCbwyjZs8pXriKPXV+muy8OTbEJXbDmltf01cHzr94gJXatxPIvu33BruuvQW3T4Kd2b9lZ0d70+m2TbnvV/DPyoneedwH8kyQT7fh5wL7pRKNAlCt8G/mdU2xlJFgFfB95Pd43fA3w93V2qES8EDqe7g3Q98JWea/JT4HE9276V7nreB/gjurt7kqTZ7cfA7Uk+leSpuesGxER9BtgM2Inu/fS90P0RSfe+/xq698J9aHkE8CngNuAhwK7Ak4GRxybHey95cjvGw9rxDgL+r53rz9sft2tzFXB5Ow6MnZccTTdaYhnde+Ye3DWa9u+AXwKLgfsDb6CrS7wQ+F/aKNyqeuc64uj1VeAxSTaf4Hvy8+nel7cDHkyXl3yCbiTND4E3reVcB9PlHPehywmO6Vl3fuvzIroc8ktJNu1ZP2ZOM8G+nw7slK7gtRFdLvtFYOuetsfS5SUPAz4PvIruOv83XZHkD0b14+ktlocBH2rX5AHtuv1Rz7bvA95XVQvb9frPtVwfzWMWMqR7OrH9wf99uv/I39az7u1VtaaqbgYOAT5eVRdW1S3A64E9eyvrwLFt+/+lu8N+8Bjnewawqqo+UVW3VdWFdH94PqdnmxOq6oKq+h3dnYjfVdWnq+p2ujeWkVEGt9P9QfzIJJtU1aqq+ilAVX2/qraeQP8/Dbwo3SiLxwMnjlp/CPCeqvpZVd3Q+v38dHd5bqV7Q3pIu0N0QVX9ZgLnXJur2vdFdInKFsA7qur3VfVduqG1vdd1otdqLN+vqv9u236Gu4oIVNWXquqqqrqjqr5INypnj559f15VH237foquuHX/iXSw/fycC+zTkqKt2zOyZ/a0PZLu5/HpdI+YfKb9vHyebrjnM3sO+cmquqwN93wqcHlVfbmqbqX7Oex9POVWYAfgAVX1u6pyElpJmuXae+teQAEfBVanGxm6zvedVoR/KnBEVV1bVbdW1elt9Uvpcptvt/e7K6vqinbcpwKvqqobq+rXdMWPkRsf472X3EpXVN8RSFX9sKqubn34XFU9egLdHclLHk73/viDUesPAd5SVb+uqtV0f/i/sOf82wI7tH6eWVU1gXOuzVV0Ixu3ZmLvyZ+oqp9W1fV0I31/WlXfae/RX2LteclXq+q8tu3xdIULAKrqs1X1f+28/0KX//Xe2Bk3p1mXlrf+L90NnF1aH28Gzupp25QudzkI+Hr7mbkVeDdwb7pCx4j3V9Uv2jGeA5xcVWe0/OcfgTt6tr0VeEiSbarqhqo6Z6Jxa36xkCHd0wFVtXVV7VBVr2j/6Y74Rc/yA+hGYQDQ/qj/P7qK+1jb/7ztM9oOwB+34XjXtSLKIUDvpI6/6lm+eYzXW7QY/oeuIv5m4NdJvpB7Pu6yVi35WEx3N+PkUf2HUf1uywvo/mj/DPAt4AtteOc72yiKDTFyPde0c/+iqnrf8H7O3a/5hK7VOHr/wL8J2DR3DcN9UbqhtiP/RjvTjXC4x75VdVNbXJ+JYkeGce5NV0SjfR9p+0VVjfwM/XzUvqOvweif0ztftwSud/1r6RKy89qw0HuMGpIkzT6tKHBoVf0R3XvSA+iK1evyQGBNVV07zrqx5sXaAdgEuLrnffAjdKM5YJz3knbD4QPAvwO/SnJcuvk91sdXgSfSjcL8zBjrx8pLRnKfd9GNZDgl3aO+r1vPc49lO7oC0nVjnHvk/NOVl9y5bZK/S/LD9kjLdXQjRsfMSxiV00zQSF6yD92NFbgrL9kHOLcVIkbnw3fQ5RkTzUtupI3SaV5KN2rjivbozTPWI2bNIxYypPXTW8W/iu6NHbjzEZH7Alf2bPPAnuXtuWt0Qa9fAKe34snI1xZV9fJJBdjd4dirxVb0PMe5Hj5LNxxz9PBNGNVvun7dBvyq3e34p6p6JF0l/hncNUnXZO+AHAj8mu6xjauAB+buc41sz92v+ZRLsgPdHa9XAvdtI1supWeuiXWYSN/PoCtY9CYMZ9E9BrJPWw/3vP5wz2vQe76r6fk5TJLe19XNRfKyqnoA8Jd0jzWN+bHBkqTZqaquAD5Jm9cLuJHu0ZERvTdHfgEsSrL1GIf6Bd1w/rHabwG26clVFlbVTu38476XVNX7q2o3usdYHkb32Mr69O0mupEML2fsQsZYeclVbd/fVtXfVdWD6EZJ/G3umjtsQ/KSC9sf4BN5T55ybT6Mv6d7nPY+LS+5nqnPS0ZupozkJWf2tI2Zl/TkGRPNSzajy5+7Dat+UlUH0xXJjgW+3HJs6W4sZEiT9zngJUmWpZtU6m101elVPdu8Jsl92nwbf0P3aMNoJwMPaxMfbdK+du+dKGmikjw8yRNbPL+jq/Tfvr7HoXvW80+5602q1+eBVydZmu6jad8GfLGqbkvyhCSPSjcp12/ohgeOnP9XdHNqTLQv90/ySrpnR1/fKvzn0iVnr23XaV+6xGT03CRTbXO6N+HVLbaXcFeyOBET6fvZdMNUX0BLGNrdstWtbeTf4r/pfl7+vE2YdRDdYycnj3Pcr9M95/pn7U7MkfQktEmem2Tk2dRrWz8n8zMjSZoh6SYJ/7uR/79bnnEwMDIMfzndo4nbt0dFXz+yb3u04xt0xYb7tPfTkUkc/4Mut3lSukkxt0uyY9vnFOBfkixs6x6c5PHt/GO+l7R85o/b6Mwb6XKTybzHvAF4/Kgca8TngX9IsjjJNsAb6W7IkOQZ6SYiDV1ecjuTyEvS2S7Jm+jmBXlDW7W+78lTZUu6m0irgQVJ3gisz0iXifT9DLrHXh5Pd2MF4BJgKd08aSN5yX8CT28/M5vQ3Qi7hS6vGcuXgWekm7vtD4C30PM3aZIXJFnc8r7rWrN5ie7BQoY0SVV1Kt1zfV+hqy4/mLueFR3xNeACuoTi63QJwujj/JZuEqvn01W1/x9dBfpekwjrXsA7gGvace5He7NNsneSGyZykOrm9Th1nOdIP053R+QMYCVdUvLXbd0f0r1B/YZuAqvTackE3eRNz0k3O/r713L665LcSPdm+TTguVX18RbX7+kmr3pq6+MHgRe1O1HTpqouB/6FboKuXwGP4q439YlYZ9/bHacL6P4NL+1ZdSbdv+MZbbv/oxvp8nd0QzFfCzyjqq4Z57jX0H2E7jva9g8dFfvuwLntZ+Mk4G+qauV69E2SNPN+SzfJ9LntPfMcuveOvwOoqm/T3TxZQffeMvoP6xfS3Wy4gm7U46vafucBL6Gb/+J6uvfxkbvtLwL+gG7yzWvp3u9HJr0e771kId2IxmvpHj/4P7o5FEhySJLLJtLZ6uaoGm8Op38GhltfL6Gb4Puf27qHAt8BbqB7D/9gVZ3W1r2drgByXZKjxjn2A1qfbqCbXPNRwL5VdUqLa73ek6fQt+iKUT+mu66/4+6Pb6zLOvteVT+m+9m4uqqua2130E1QvpBWqKiqH9HdcPk3utzsmXQTif5+jMNSVZcBf0V3Q/Bqup+NX/Zs8hTgsnbd3wc8v7p5z6S7yYbPdyNJkiRJkjQzHJEhSZIkSZIGhoUMSZIkSZI0MCxkSJIkSZKkgWEhQ5IkSZIkDQwLGZIkSZIkaWAs6HcAg26bbbapJUuW9DsMSZLGdMEFF1xTVYv7HYf6wzxFkjSbTTZPsZCxgZYsWcLw8HC/w5AkaUxJft7vGNQ/5imSpNlssnmKhYwNtHz5lSxadHS/w5AkzRFr1hzT7xAkaVKWnriq3yFImiecI0OSJEmSJA0MCxmSJEmSJGlgWMiQJEmSJEkDw0KGJEnSOJIcmeSHSY6f4uOelmRoKo8pSdJ8MWcn+0yyoKpu63cckiRpoL0CeGpVrRxpMMeQJKm/BnpERpIXJVmR5OIkn0nyySTvSfI94NgkeyQ5O8lF7fvD236HJvlqkm8m+UmSd/Yc88lJfpDkwiRfSrJF3zooSZL6JsmHgQcBJyW5PslxSU4BPp1kcZKvJDm/fT2u7bN5ko+3touS7N/a753kCy1v+SJw757zHJzkkiSXJjm2p/2GJMcmuSDJd1pec1qSnyV51sxeDUmSZo+BHZGRZCfgaOBxVXVNkkXAe4CHAftV1e1JFgL7VNVtSfYD3gY8ux1iGbArcAvwoyT/BtwM/EPb/8Ykfw/8LfCWmeybJEnqv6o6IslTgCcArwSeCexVVTcn+Rzw3qr6fpLtgW8Bj6DLTb5bVYcl2Ro4L8l3gL8EbqqqRyd5NHAhQJIHAMcCuwHXAqckOaCqTgQ2B06rqr9PcgLwz8CfAo8EPgWcNDNXQpKk2WVgCxnAE4EvV9U1AFW1JgnAl6rq9rbNVsCnkjwUKGCTnv1PrarrAZJcDuwAbE2XHJzVjvUHwA9GnzjJ4cDhABtttHDKOyZJkmalk6rq5ra8H/DIli8ALEyyJfBk4FlJjmrtmwLbA/sA7weoqhVJVrT1u9MVK1YDtLk49gFOBH4PfLNtdwlwS1XdmuQSYMl4QfbmKdtvv/2G9FeSpFlpkAsZoStOjHZjz/Jbge9V1YFJlgCn9ay7pWf5drprEeDbVXXw2k5cVccBxwEsWLDtWDFIkqS5pzfH2AjYs6ewAUC6ysazq+pHo9ph7LwlY7SNuLWqRva5g5a7VNUdScbN4XrzlKGhIfMUSdKcM8hzZJwKPC/JfQHaoyWjbQVc2ZYPncAxzwEel+Qh7ZibJXnYFMQqSZLmllPoHjcBIMmytvgt4K9bQYMku7b2M4BDWtvOwKNb+7nA45Nsk2Rj4GDg9GmPXpKkATawhYyqugw4Bjg9ycV082OM9k7g7UnOAjaewDFX0xU8Pt+GfJ4D7DhlQUuSpLniSGCoTd55OXBEa38r3aOsK5Jc2l4DfAjYouUXrwXOA6iqq4HXA98DLgYurKqvzVw3JEkaPLlrxKImY8GCbWvhwsP6HYYkaY5Ys+aYKT1ekguqamhKD6qBMTQ0VMPDw/0OQ/PE0hNX9TsESQNm1YFLJ5WnDOyIDEmSJEmSNP9YyJAkSZIkSQPDQoYkSZIkSRoYg/zxq7PCsmXbMTw8tc8zS5IkSYNm5QFL+h2CpAGzts8gXxtHZEiSJEmSpIFhIUOSJEmSJA0MCxmSJEmSJGlgOEfGBlq+/EoWLTq632FIkmaJNWucN0nS/LT0xFX9DkHSPOGIDEmSJEmSNDAsZEiSJEmSpIFhIUOSJEmSJA2MeVPISLJvkpPHWbcqyTYzHZMkSdJkJHlzkqP6HYckSf0wbwoZkiRJkiRp8M36QkaSJUku7Xl9VLsLcVqSY5Ocl+THSfbu2f7MJBe2r8f2HG5hkhOSXJ7kw0nu0f8kL2jHXJ7kI0k2noFuSpKkAdLyjSuSfCrJiiRfTrJZkjcmOT/JpUmOS5K2/ZEt/1iR5Aut7fEt31ie5KIkW7b217RjrEjyTz3nPDrJj5J8B3h4XzouSdIsMOsLGeuwoKr2AF4FvKm1/Rr406p6DHAQ8P6e7fcA/g54FPBg4M96D5bkEW2fx1XVMuB24JBpjF+SJA2uhwPHVdWjgd8ArwA+UFW7V9XOwL2BZ7RtXwfs2rY9orUdBfxVyzn2Bm5O8mTgoXQ5yzJgtyT7JNkNeD6wK13+svsM9E+SpFlpQb8D2EBfbd8vAJa05U2ADyRZRleIeFjP9udV1c8Aknwe2Av4cs/6JwG7Aee3Gyj3piuM3E2Sw4HDATbaaOHU9ESSJA2aX1TVWW35s8CRwMokrwU2AxYBlwH/BawAjk9yInBi2+cs4D1Jjge+WlW/bIWMJwMXtW22oCtsbAmcUFU3ASQ5abygevOU7bfffmp6KknSLDIIIzJu4+5xbtqzfEv7fjt3FWVeDfwK2AUYAv6gZ/sadezRrwN8qqqWta+HV9WbRwdUVcdV1VBVDSWbrVdnJEnSnDFWXvFB4DlV9Sjgo9yVtzwd+He6GyYXJFlQVe8A/oLuxsk5SXaky0Xe3pOLPKSq/mOc840dVE+esnjx4g3qoCRJs9EgFDJ+BdwvyX2T3Iu7hmiOZyvg6qq6A3gh0DvHxR5Jlra5MQ4Cvj9q31OB5yS5H0CSRUl2mJJeSJKkuWb7JHu25YO5K6+4JskWwHMAWt7xwKr6HvBaYGtgiyQPrqpLqupYYBjYEfgWcFjbnyTbtbzkDODAJPduc2k8c2a6KEnS7DPrHy2pqluTvAU4F1gJXLGOXT4IfCXJc4HvATf2rPsB8A66OTLOAE4Yda7Lk/wDcEpLOm4F/gr4+VT0RZIkzSk/BF6c5CPAT4APAfcBLgFWAee37TYGPptkK7oRF++tquuSvDXJE+hGll4OfKOqbmlzdv2gPeZ6A/CCqrowyReB5XR5yZkz1EdJkmadVE1olKLGsWDBtrVw4WH9DkOSNEusWXNMv0O4myQXVNVQv+OYa5IsAU5uk3rOWkNDQzU8PNzvMDRPLD1xVb9DkDRgVh24dFJ5yiA8WiJJkiRJkgQMwKMlkiRJs01VrQJm9WgMSZLmKkdkSJIkSZKkgeGIjA20bNl2DA/PruehJUmSpJm28oAl/Q5B0oDJJPdzRIYkSZIkSRoYFjIkSZIkSdLAsJAhSZIkSZIGhnNkbKDly69k0aKj+x2GJM1La9Y4R5EkzRZLT1zV7xAkzROOyJAkSZIkSQPDQoYkSZIkSRoYFjIkSZIkSdLAsJAhSZIkSZIGhoUMSZIkSZI0MOZlISPJxv2OQZIkSZIkrb85WchIcmKSC5JcluTw1nZDkrckORfYM8k7klyeZEWSd7dtFif5SpLz29fj+toRSZI0LyX5xyRXJPl2ks8nOSrJaUn+NcnZSS5Nske/45QkqR8W9DuAaXJYVa1Jcm/g/CRfATYHLq2qNyZZBPwHsGNVVZKt237vA95bVd9Psj3wLeARow/eiiOHA2y00cIZ6I4kSZovkgwBzwZ2pcvVLgQuaKs3r6rHJtkH+Diw8xj735mnbL/99jMSsyRJM2muFjKOTHJgW34g8FDgduArre03wO+AjyX5OnBya98PeGSSkeMsTLJlVf229+BVdRxwHMCCBdvWtPVCkiTNR3sBX6uqmwGS/FfPus8DVNUZSRYm2bqqruvduTdPGRoaMk+RJM05c66QkWRfuoLEnlV1U5LTgE2B31XV7QBVdVsbjvkk4PnAK4En0j1qs+dI4iBJktQHWcu60YUJCxWSpHlnLs6RsRVwbSti7Aj8yegNkmwBbFVV/w28CljWVp1CV9QY2W7Z6H0lSZKm2feBZybZtOUsT+9ZdxBAkr2A66vq+n4EKElSP825ERnAN4EjkqwAfgScM8Y2WwJfS7Ip3V2PV7f2I4F/b/suAM4Ajpj+kCVJkjpVdX6Sk4CLgZ8Dw8BIweLaJGcDC4HD+hSiJEl9NecKGVV1C/DUMVZt0bPN1cA9ZvquqmtodzokSZL66N1V9eYkm9HdWPkX4BDgK1X1+v6GJklSf825QoYkSdIccFySR9LN8/WpqrqwZzJySZLmNQsZkiRJs0xV/fkYbfv2IRRJkmYdCxkbaNmy7RgePqbfYUiSJEl9tfKAJf0OQdKAmexYw7n4qSWSJEmSJGmOspAhSZIkSZIGhoUMSZIkSZI0MJwjYwMtX34lixYd3e8wJGleWrPGOYokabZYeuKqfocgaZ5wRIYkSZIkSRoYFjIkSZIkSdLAsJAhSZIkSZIGhoUMSZIkSZI0MOZsISPJvklO7ncckiRJE5VkKMn717HNsiRPm6mYJEmabfzUEkmSpFmiqoaB4XVstgwYAv572gOSJGkWGpgRGUmWJLm05/VRSd6c5LQkxyY5L8mPk+w9xr6bJ/l4kvOTXJRk/9a+U9tveZIVSR7atv16kouTXJrkoJnspyRJmluSHJ3kR0m+k+TzLYc5LclQW79NklVt+c4RpWPlL0n+AHgLcFDLX8xTJEnzzlwZkbGgqvZowyzfBOw3av3RwHer6rAkWwPnJfkOcATwvqo6viUGGwNPA66qqqcDJNlq9MmSHA4cDrDRRgunq0+SJGnAJdkNeD6wK13edSFwwQR3v0f+AnwHeCMwVFWvHOecd+Yp22+//QbFL0nSbDQwIzLW4avt+wXAkjHWPxl4XZLlwGnApsD2wA+ANyT5e2CHqroZuATYr43y2Luqrh99sKo6rqqGqmoo2WzKOyNJkuaMvYETquqmqvoNcNJ67Dte/rJWvXnK4sWLJxGyJEmz2yCNyLiNuxdeNu1ZvqV9v52x+xTg2VX1o1HtP0xyLvB04FtJ/qKqvtvunjwNeHuSU6rqLVPTBUmSNA/VGG29ec2mY6yHcfKXJH88hbFJkjRwBmlExq+A+yW5b5J7Ac9Yj32/Bfx1kgAk2bV9fxDws6p6P90dkkcneQBwU1V9Fng38Jip7IQkSZpXzgAOTHLvJFsCz2ztq4Dd2vJzxtl3zPwF+C2w5fSEK0nS7DcwhYyqupVucqtzgZOBK9Zj97cCmwAr2oShb23tBwGXtiGbOwKfBh5FN4fGcrpnU/95KuKXJEnzT1VdCHwRWA58BTizrXo38PIkZwPbjLP7ePnL94BHOtmnJGm+StVYox01UQsWbFsLFx7W7zAkaV5as+aYfocw6yW5oKqG+h2HOkneDNxQVe+eifMNDQ3V8PC6Ps1VmhpLT1zV7xAkDZhVBy6dVJ4yMCMyJEmSJEmSBmmyT0mSpIFWVW/udwySJA06CxkbaNmy7RgedmizJEmS5reVByzpdwiSBkwmuZ+PlkiSJEmSpIFhIUOSJEmSJA0MCxmSJEmSJGlgOEfGBlq+/EoWLTq632FI0qznR6VK0tzmx69KmimOyJAkSZIkSQPDQoYkSZIkSRoYFjIkSZIkSdLAsJAhSZIkSZIGxrwqZCT57yRbt+Ujk/wwyfFJnpXkdX0OT5IkDZgkhyZ5wBQcZ1WSbaYiJkmS5rp59aklVfW0npevAJ5aVSvb65P6EJIkSRpshwKXAldNdIckC6rqtmmLSJKkOW7WjchIsnmSrye5OMmlSQ5qdymOTXJe+3pI23Zxkq8kOb99Pa61b5HkE0kuSbIiybNb+6ok2yT5MPAg4KQkr253Uz7Qtrl/khPa+S9O8th+XQtJkjTzxslF3thyjUuTHJfOc4Ah4Pgky5Pcu3dkRZKhJKe15Te3/U4BPp3kvklOSXJRko8A6Tn/iUkuSHJZksNb20uTvLdnm5clec8MXhZJkmaNWVfIAJ4CXFVVu1TVzsA3W/tvqmoP4APAv7a29wHvrardgWcDH2vt/whcX1WPqqpHA9/tPUFVHUF35+QJVfVe7u79wOlVtQvwGOCy0QEmOTzJcJLhqps2sLuSJGmWGSsX+UBV7d5e3xt4RlV9GRgGDqmqZVV18zqOuxuwf1X9OfAm4PtVtSvdqNDte7Y7rKp2oyuSHJnkvsAXgGcl2aRt8xLgE2OdpDdPWb169WT6L0nSrDYbCxmXAPu1ERh7V9X1rf3zPd/3bMv7AR9IspwuCViYZMvW/u8jB6yqa9fj/E8EPtT2u73n/HeqquOqaqiqhpLN1uPQkiRpAIyVizwhyblJLqHLFXaaxHFP6il27AN8FqCqvg705ipHJrkYOAd4IPDQqrqR7sbMM5LsCGxSVZeMdZLePGXx4sWTCFOSpNlt1s2RUVU/TrIb8DTg7W0IJkD1bta+bwTsOfoOSJKM2l6SJGlCxslF/goYqqpfJHkzsOk4u9/GXTeKRm9z4+hTjd45yb50N2T2rKqb2qMpI8f5GPAG4ArGGY0hSdJ8MOtGZLSZv2+qqs8C76Z7vAPgoJ7vP2jLpwCv7Nl32Tjt91mPEE4FXt722zjJwvXsgiRJGmBryUWuSbIF8JyezX8LbNnzehXdIyTQPfY6njOAQ9r5ngqM5CpbAde2IsaOwJ+M7FBV59KN0Phz7hqpKknSvDPrChnAo4Dz2uMiRwP/3NrvleRc4G+AV7e2I4GhNqHn5cARrf2fgfu0CbkuBp6wHuf/G7rho5cAFzC5oaOSJGlwjZWLfJTukZMTgfN7tv0k8OGRyT6BfwLel+RM4Pa1nOOfgH2SXAg8Gfjf1v5NYEGSFcBb6R4v6fWfwFnr+disJElzSqpm/xMYSVbRDee8pt+xjLZgwba1cOFh/Q5Dkma9NWuO6XcI81KSC6pqqN9xaGokOZluovNTJ7L90NBQDQ8PT3NUUmfpiav6HYKkAbPqwKWTylNm44gMSZIk9UiydZIfAzdPtIghSdJcNesm+xxLVS3pdwySJEn9UlXXAQ/rdxySJM0GA1HImM2WLduO4WGHS0uSJGl+W3nAkn6HIGnAZJL7+WiJJEmSJEkaGBYyJEmSJEnSwLCQIUmSJEmSBoZzZGyg5cuvZNGio/sdhiTNen78qiTNbX78qqSZ4ogMSZIkSZI0MCxkSJIkSZKkgWEhQ5IkSZIkDQwLGZIkSZIkaWDM2UJGkiOT/DDJ8euxz38n2bp9vWI645MkSfNLkiVJLp2iY+2b5OSpOJYkSYNmzhYygFcAT6uqQ0Yakqz1U1qq6mlVdR2wddtfkiRJkiTNInOykJHkw8CDgJOSXJ/kuCSnAJ9OcmiSD/Rse3KSfdvyqiTbAO8AHpxkeZJ39aELkiRpFknygiTntdzgI0k2TnJDkmOSXJzknCT3b9s+uL0+P8lbktwwxvGWJDkzyYXt67Gtfd8kpyX5cpIrkhyfJG3dU1rb94E/m9ELIEnSLDInCxlVdQRwFfAE4L3AbsD+VfXnEzzE64CfVtWyqnrN6JVJDk8ynGS46qYpi1uSJM0+SR4BHAQ8rqqWAbcDhwCbA+dU1S7AGcDL2i7vA95XVbvT5SNj+TXwp1X1mHbs9/es2xV4FfBIuhszj0uyKfBR4JnA3sAfriXeO/OU1atXr3+HJUma5eZkIWMMJ1XVzVN1sKo6rqqGqmoo2WyqDitJkmanJ9HdFDk/yfL2+kHA74GReSouAJa05T2BL7Xlz41zzE2Ajya5pG37yJ5151XVL6vqDmB5O+6OwMqq+klVFfDZ8YLtzVMWL168Ht2UJGkwrHXOiDnkxp7l27h7AWfTGY5FkiQNlgCfqqrX360xOaoVFaAbpbE+edWrgV8Bu9DlJb/rWXdLz3LvcQtJkjRvRmT0WgUsS7JRkgcCe4yxzW+BLWc0KkmSNFudCjwnyf0AkixKssNatj8HeHZbfv4422wFXN1GXbwQ2HgdMVwBLE3y4Pb64AlFLknSHDQfCxlnASuBS4B3AxeO3qCq/g84K8mlTvYpSdL8VlWXA/8AnJJkBfBtYNu17PIq4G+TnNe2u36MbT4IvDjJOcDDuPvo0bFi+B1wOPD1Ntnnz9e3H5IkzRW5a0SkJmPBgm1r4cLD+h2GJM16a9Yc0+8Q5qUkF1TVUL/jmE/STaB1c1VVkucDB1fV/v2IZWhoqIaHh/txas1DS09c1e8QJA2YVQcunVSeMl/myJAkSZopuwEfaB+beh3gHQ9JkqaQhQxJkqQpVFVn0k3iKUmSpsF8nCNDkiRJkiQNKEdkbKBly7ZjeNjnviVJkjS/rTxgSb9DkDRgMsn9HJEhSZIkSZIGhoUMSZIkSZI0MCxkSJIkSZKkgeEcGRto+fIrWbTo6H6HIUmz3po1zickSXPB0hNX9TsESfOcIzIkSZIkSdLAsJAhSZIkSZIGhoUMSZIkSZI0MOZ0ISPJx5I8st9xSJIkTbUkN/Q7BkmS+mFOT/ZZVX/R7xgkSZJGS7JxVd3e7zgkSRpEc2JERpIlSa5I8qkkK5J8OclmSU5LMpTkWUmWt68fJVnZ2kfaLklS7VgvS3J+kouTfCXJZv3unyRJGhxryUtWJXljku8Dz01ycMtBLk1ybM/+N/QsPyfJJ9vy0iQ/aHnKW2e+Z5IkzQ5zopDRPBw4rqoeDfwGeMXIiqo6qaqWVdUy4GLg3VU13NP2TeDdbfOvVtXuVbUL8EPgpTPZCUmSNCeMl5f8rqr2As4AjgWeCCwDdk9ywDqO+T7gQ1W1O/D/piNoSZIGwVwqZPyiqs5qy58F9hq9QZLXAjdX1b/3tD0PeAzwuta0c5Izk1wCHALsNMZxDk8ynGS46qap7ockSRp84+UlX2zfdwdOq6rVVXUbcDywzzqO+Tjg8235M+Nt1JunrF69enLRS5I0i82lQkat7XWSJwHPBY7oadsJ+Cfg+T3PqX4SeGVVPaqt2/QeJ6o6rqqGqmrIJ08kSdIYxstLbmzfM8F9R+cho497z5178pTFixeva3NJkgbOXCpkbJ9kz7Z8MPD9kRVJdgA+CDyvqm5ubVsBXwBeVFW9tyu2BK5OsgndiAxJkqT1NW5e0pwLPD7JNkk2btuc3tb9KskjkmwEHNizz1nA89uyOYokad6aS4WMHwIvTrICWAR8qGfdocB9gRPa5J7/DRwA7AB8dGTSz7btP9IlF98GrpiZ0CVJ0hyztryEqroaeD3wPbr5uy6sqq+11a8DTga+C1zds9vfAH+V5Hxgq+kNX5Kk2WsuffzqHVV1xKi2fdv3YbrHREb71OiGqvoQo5INSZKk9TRWXrKk90VVfQ743Ogdq+rLwJfHaF8J7NnT9I4ND1OSpMEzl0ZkSJIkSZKkOW5OjMioqlXAzv2OQ5IkybxEkqTp5YgMSZIkSZI0MObEiIx+WrZsO4aHj+l3GJIkSdKMWHnAkn6HIGmOWNtnka+NIzIkSZIkSdLAsJAhSZIkSZIGhoUMSZIkSZI0MJwjYwMtX34lixYd3e8wJGnWW7PG+YQkadAtPXFVv0OQJEdkSJIkSZKkwWEhQ5IkSZIkDQwLGZIkSZIkaWDM2UJGkiOSvGiM9iVJLu1HTJIkaf5KsnWSV0xy332TnDzVMUmSNIjmbCGjqj5cVZ/e0OMk2Xgq4pEkSfPe1sCkChmSJOkuA1XISLJ5kq8nuTjJpUkOSrIqybFJzmtfD2nbvjnJUW15t7bPD4C/6jnexkneleT8JCuS/GVr3zfJ95J8DrikH32VJElzzjuABydZ3vKPd7V85pIkBwGkc4/2Xkl2T3JRkgfNeA8kSZoFBqqQATwFuKqqdqmqnYFvtvbfVNUewAeAfx1jv08AR1bVnqPaXwpcX1W7A7sDL0uytK3bAzi6qh451Z2QJEnz0uuAn1bVMuAcYBmwC7Af8K4k2wJ/Nk47AEkeC3wY2L+qfjaTwUuSNFsMWiHjEmC/NgJj76q6vrV/vuf73YoVSbYCtq6q01vTZ3pWPxl4UZLlwLnAfYGHtnXnVdXKsYJIcniS4STDVTdtcKckSdK8sxfw+aq6vap+BZxOd1NlvHaARwDHAc+sqv8d78C9ecrq1auntxeSJPXBQBUyqurHwG50BY23J3njyKrezUbtljHaetf9dVUta19Lq+qUtu7GtcRxXFUNVdVQstn6d0SSJM13Wc92gKuB3wG7ru3AvXnK4sWLJxufJEmz1kAVMpI8ALipqj4LvBt4TFt1UM/3H/TuU1XXAdcn2as1HdKz+lvAy5Ns0o7/sCSbT1P4kiRpfvstsGVbPgM4qM3XtRjYBzhvLe0A1wFPB96WZN8ZjFuSpFllQb8DWE+PontW9A7gVuDlwJeBeyU5l64wc/AY+70E+HiSm+iKFyM+BiwBLkwSYDVwwLRFL0mS5q2q+r8kZ7WPgf8GsAK4mG7k6Gur6v8lOYHuMdnR7Tu2Y/wqyTOBbyQ5rKrO7U9vJEnqn1SN99TFYEiyChiqqmv6cf4FC7athQsP68epJWmgrFlzTL9DmJeSXFBVQ/2OQ/0xNDRUw8PD/Q5Dc8jSE1f1OwRJc8iqA5dOKk8ZqEdLJEmSJEnS/DZoj5bcQ1Ut6XcMkiRJkiRpZjgiQ5IkSZIkDYyBH5HRb8uWbcfwsM99S5Ikae5becCSfocgaQ5Z22eOr40jMiRJkiRJ0sCwkCFJkiRJkgaGhQxJkiRJkjQwnCNjAy1ffiWLFh3d7zAkaVZas8Y5hCRpUC09cVW/Q5CkMTkiQ5IkSZIkDQwLGZIkSZIkaWBYyJAkSZIkSQPDQoYkSZIkSRoYs7aQkeTIJD9Mcvx67vfJJM8Zo/1jSR45dRFKkiT1T5LTkgz1Ow5JkmbabP7UklcAT62qlVNxsKr6i6k4jiRJkiRJ6p9ZOSIjyYeBBwEnJbk+yVE96y5NsqQtvyjJiiQXJ/nMGMd5axuhsVHvXYskNyQ5pu13TpL7t/YHt9fnJ3lLkhtmpMOSJKnvkixJckUbxXlpkuOT7JfkrCQ/SbJH+zo7yUXt+8Pbvocm+WqSb7Zt39lz3A8lGU5yWZJ/6ml/Wjvf95O8P8nJrX3zJB9v+chFSfZv7fdO8oWW+3wRuPcMXyJJkmaFWVnIqKojgKuAJwDvHWubJDsBRwNPrKpdgL8Ztf6dwP2Al1TVHaN23xw4p+13BvCy1v4+4H1VtXs7/5iSHN4SkuGqm9a7f5IkadZ6CF0+8GhgR+DPgb2Ao4A3AFcA+1TVrsAbgbf17LsMOAh4FHBQkge29qOraqgd8/FJHp1kU+AjdKNP9wIW9xznaOC7LR95AvCuJJsDLwduqqpHA8cAu43Vgd48ZfXq1Rt2NSRJmoVmZSFjgp4IfLmqrgGoqjU96/4R2Lqq/rKqaox9fw+c3JYvAJa05T2BL7Xlz4134qo6rqqGqmoo2WwDuiBJkmaZlVV1SbsJchlwasslLqHLF7YCvpTkUrqbLTv17HtqVV1fVb8DLgd2aO3PS3IhcFHb/pF0RZKf9TxC+/me4zwZeF2S5cBpwKbA9sA+wGcBqmoFsGKsDvTmKYsXLx5rE0mSBtpsniNjxG3cveCyafseYKwiBcD5wG5JFo0qcIy4tafAcTuDcR0kSdL0u6Vn+Y6e13fQ5QtvBb5XVQe2R11PG2ff24EFSZbSjebYvaquTfJJulwma4khwLOr6kd3a0xg/NxHkqR5YxBGZKwCHgOQ5DHA0tZ+Kt0djvu2dYt69vkm8A7g60m2XI9znQM8uy0/fwNiliRJc9NWwJVt+dAJbL8QuBG4vs3J9dTWfgXwoJF5v+geSRnxLeCv0yoXSXZt7WcAh7S2nekeVZEkad4ZhELGV4BFbXjly4EfA1TVZXTPh56e5GLgPb07VdWXgI/STRg60cmwXgX8bZLzgG2B66eiA5Ikac54J/D2JGcBG69r46q6mO6RksuAjwNntfab6T6h7ZtJvg/8irvyjrcCmwAr2iMsb23tHwK2SLICeC1w3lR1SpKkQZKxp5CYn9JNeHFzVVWS5wMHV9X+a9tnwYJta+HCw2YmQEkaMGvWHNPvEOa9JBe0iSY1yyTZoqpuaCMv/h34SVWNOcn5ZA0NDdXw8PBUHlLzyNITV/U7BElz3KoDl04qT3FuiLvbDfhASyiuA6xQSJKk6fKyJC8G/oBu1MZH+hyPJEkDwUJGj6o6E9il33FIkqS5r42+mNIRGJIkzQcWMjbQsmXbMTzs0GlJkiTNLSsPWNLvECTNcWv7CK+1GYTJPiVJkiRJkgALGZIkSZIkaYBYyJAkSZIkSQPDOTI20PLlV7Jo0dH9DkOSZpwfrSpJc5sfvypptnJEhiRJkiRJGhgWMiRJkiRJ0sCwkCFJkiRJkgaGhQxJkiRJkjQwLGRIkiRNgSRHJvlhkmuTvG4DjnND+74kyaVTF6EkSXPDvPzUkiQbV9Xt/Y5DkiTNKa8AnlpVK/sdiCRJc9nAjchI8tokR7bl9yb5blt+UpLPJjk4ySVJLk1ybM9+NyR5S5JzgT2TvCPJ5UlWJHl322Zxkq8kOb99Pa4vnZQkSQMlyYeBBwEnJXl1kg+09k8meX+Ss5P8LMlzWvsWSU5NcmHLW/Zfx/HPTLKs5/VZSR49jV2SJGnWGrhCBnAGsHdbHgK2SLIJsBfwE+BY4InAMmD3JAe0bTcHLq2qPwYuBw4EdqqqRwP/3LZ5H/DeqtodeDbwsbECSHJ4kuEkw1U3TXH3JEnSoKmqI4CrgCcA145avS1dnvIM4B2t7XfAgVX1mLbPvyTJWk7xMeBQgCQPA+5VVSvG2rA3T1m9evUkeyRJ0uy11kdLkvwXUOOtr6pnTXlE63YBsFuSLYFbgAvpChp7A/8FnFZVqwGSHA/sA5wI3A58pR3jN3QJxMeSfB04ubXvBzyyJ49YmGTLqvptbwBVdRxwHMCCBduOe30kSdL0maV5ylhOrKo7gMuT3L+1BXhbkn2AO4DtgPsD/2+cY3wJ+MckrwEOAz453sl685ShoSHzFEnSnLOuOTLe3b7/GfCHwGfb64OBVdMU01pV1a1JVgEvAc4GVtDdyXgw8L/AbuPs+ruReTGq6rYkewBPAp4PvJJuFMdGwJ5VdfO0dkKSJE2FWZenjOOWnuWRuyWHAIuB3Xpym03HO0BV3ZTk28D+wPPobuJIkjQvrbWQUVWnAyR5a1Xt07Pqv5KcMa2Rrd0ZwFF0dyQuAd5DN1LjHOBfk2xDN6zzYODfRu+cZAtgs6r67yTnAP/TVp1CV9R4V9tuWVUtn96uSJKkyZjFecpEbAX8uhUxngDsMIF9PkY3+vTMqlozrdFJkjSLTXSOjMVJHjTyIslSursI/XIm3fOmP6iqX9E9JnJmVV0NvB74HnAxcGFVfW2M/bcETk6yAjgdeHVrPxIYahOAXg4cMc39kCRJG2625SkTcTxdzjFMNzrjinXtUFUX0D0e+4lpjk2SpFltoh+/+irgtCQ/a6+XAIdPR0ATUVWnApv0vH5Yz/LngM+Nsc8WPctXA3uMsc01wEFTHa8kSZpWr2IW5ClVtaQtfrJ9UVWHjtpmi/b9GmDPcY4zss0qYOeR9iQPoLsJdcoUhi1J0sBZZyEjyUZ0wx8fCuzYmq+oqlvG30uSJGn6zZc8JcmLgGOAv20Th0qSNG+t89GS9mb5yqq6paoubl9zKjmQJEmDab7kKVX16ap6YFV9qd+xSJLUbxN9tOTbSY4CvgjcONLoRFOwbNl2DA8f0+8wJEmaz8xTpGmw8oAl/Q5B0hyXdW8ypokWMg5r3/+qp62AB42xrSRJ0kwyT5EkaR6ZUCGjqpZOdyCSJEmTYZ4iSdL8MqFCRpJNgJcDI5/Rfhrwkaq6dZrikiRJmhDzFEmS5peJPlryIbqPO/1ge/3C1vYX0xHUIFm+/EoWLTq632FI0gZbs8b5fjSwzFOkabD0xFX9DkGSxjTRQsbuVbVLz+vvJrl4OgKSJElaT+YpkiTNI+v8+NXm9iQPHnmR5EHA7dMTkiRJ0noxT5EkaR5Z64iMJK8CzgJeR3d3Y2VbtYS7ZgiXJEmaceYpkiTNT+t6tOSPgPcBjwB+DKwBLgA+UVVXTXNskiRJa2OeIknSPLTWR0uq6qiqeixwf+DVwNnAY4DhJJfPQHwzIsm+SU7udxySJGni5kueIkmS7m6ik33eG1gIbNW+rgIuma6gJEmS1sOcz1OSLKiq2/odhyRJs8G65sg4DtgJ+C1wLt2djvdU1bVTGUSSzYH/pBsiujHwVuB/gPcAWwDXAIdW1dVJHgJ8GFhMN5HXc4GfAe8EngoU8M9V9cUk+wJvbvvvTDfc9AVVVUmeAvxrW3dhTyyPpxumSjvWPlX126nsryRJ2nAzladMpSQvAI4E/oAu5lcA11fVFm39c4BnVNWhST5J97jMrsCFST5DlwNtBvwUOGw291WSpOmyrhEZ2wP3An4CXAn8ErhuGuJ4CnBVVT0dIMlWwDeA/atqdZKDgGPoJu46HnhHVZ2QZFO6x2P+DFgG7AJsA5yf5Ix27F3pkpyr6CYEe1ySYeCjwBPpCiZf7InlKOCvquqsJFsAvxsdbJLDgcMBNtpo4ZRdBEmStF5mKk+ZEkkeARwEPK6qbk3yQeCQdez2MGC/qro9yQrgr6vq9CRvAd4EvGqM89yZp2y//fZT2QVJkmaFtRYyquopSUJXCHgs8HfAzknWAD+oqjdNURyXAO9OcixwMnAt3QiKb3enZ2Pg6iRbAttV1Qktvt8BJNkL+HxV3Q78KsnpwO7Ab4DzquqXbbvldDOZ3wCsrKqftPbP0t7w6Yod70lyPPDVkX1HXZfjgOMAFizYtqboGkiSpPUwg3nKVHkSsBvdDRfoHon59Tr2+VIrYmwFbF1Vp7f2TwFfGmuH3jxlaGjIPEWSNOesc46Mqirg0iTXAde3r2cAe9DdCdhgVfXjJLsBTwPeDnwbuKyq9uzdLsl4wx+ylsPf0rN8O3f1ecw39qp6R5Kvt1jOSbJfVV0xgW5IkqQZNhN5yhQK8Kmqev3dGpO/63m56ah9bpz2qCRJGjBr/dSSJEcm+UKSXwBn0CUGP6J7lGPRVAWR5AHATVX1WeDdwB8Di5Ps2dZvkmSnqvoN8MskB7T2eyXZrMV2UJKNkywG9gHOW8sprwCWJnlwe31wTywPrqpLqupYYBjYcar6KUmSps5M5SlT6FTgOUnuB5BkUZId6EaTPiLJRsCBY+1YVdcD1ybZuzW9EDh9rG0lSZrr1jUiYwnwZeDVVXX1NMbxKOBdSe4AbgVeDtwGvL8NpVxANzHnZXRv3B9pz4beSjfZ5wnAnsDFdCMtXltV/y/JmEWIqvpde37060muAb5P9ygLwKuSPIFu9MbldHN1SJKk2WcJM5OnTImqujzJPwCntKLFrcBfAa+je7T2F8CldBOdj+XFwIfbTZyfAS+Z/qglSZp90o3I1GQtWLBtLVx4WL/DkKQNtmbNMf0OQdMgyQVVNdTvONQfQ0NDNTw83O8wNKCWnriq3yFImuNWHbh0UnnKWh8tkSRJkiRJmk0sZEiSJEmSpIGxzk8t0dotW7Ydw8MOx5YkSdLcsvKAJf0OQdIct7aPH10bR2RIkiRJkqSBYSFDkiRJkiQNDAsZkiRJkiRpYDhHxgZavvxKFi06ut9hSNIG8+NXJUm9/PhVSbOVIzIkSZIkSdLAsJAhSZIkSZIGhoUMSZIkSZI0MCxkSJIkSZKkgTHvCxlJliS5tN9xSJIkSZKkdZv3hQxJkiRJkjQ4+l7ISPKCJOclWZ7kI0n+OMmKJJsm2TzJZUl2TrJFklOTXJjkkiT7t/2XJLkiyceSXJrk+CT7JTkryU+S7NG2e3OSzyT5bmt/2RixbJzkXUnObzH85UxfD0mSNH+0XOfrSS5uecxBSZ6U5KKW73w8yb3atquS/FNPLrRjv+OXJKkf+lrISPII4CDgcVW1DLgdeDhwEvDPwDuBz1bVpcDvgAOr6jHAE4B/SZJ2qIcA7wMeDewI/DmwF3AU8IaeUz4aeDqwJ/DGJA8YFdJLgeurandgd+BlSZaOEffhSYaTDFfdtIFXQZIkzWNPAa6qql2qamfgm8AngYOq6lHAAuDlPdtf03KhD9HlOffQm6esXr16eqOXJKkP+j0i40nAbsD5SZa31w8C3gL8KTBEV8wACPC2JCuA7wDbAfdv61ZW1SVVdQdwGXBqVRVwCbCk53xfq6qbq+oa4HvAHqPieTLwohbLucB9gYeODrqqjquqoaoaSjbbgO5LkqR57hJgvyTHJtmbLm9ZWVU/bus/BezTs/1X2/cLuHuOc6fePGXx4sXTE7UkSX20oM/nD/Cpqnr93RqTPwS2ADYBNgVuBA4BFgO7VdWtSVa1dQC39Ox+R8/rO7h7H2vU+Ue/DvDXVfWtSfVGkiRpPVTVj5PsBjwNeDtwyjp2Gclxbqf/eZwkSX3R7xEZpwLPSXI/gCSLkuwAHAf8I3A8cGzbdivg162I8QRgh0mcb/8298Z9gX2B80et/xbw8iSbtHgelmTzSZxHkiRpndpjrjdV1WeBdwOPBZYkeUjb5IXA6f2KT5Kk2aivlfyqujzJPwCnJNkIuBX4GnBbVX0uycbA2UmeSFfU+K8kw8By4IpJnPI84OvA9sBbq+qqJEt61n+MbpjmhW3+jdXAAZPpmyRJ0gQ8CnhXkjvo8qCX0928+VKSBXQ3XT7cx/gkSZp1+j4ksaq+CHxxnHW3A3/c07TnOIfZuWefQ3uWV/WuA35cVYePOsed27Q5Nt7A3ScIlSRJmhbtcdaxHmnddYxtl/QsD9ONLpUkad7p96MlkiRJkiRJE9b3ERkzpare3O8YJEmSJEnShnFEhiRJkiRJGhjzZkTGdFm2bDuGh4/pdxiSJEnSlFp5wJJ+hyBpjssk93NEhiRJkiRJGhgWMiRJkiRJ0sCwkCFJkiRJkgaGc2RsoOXLr2TRoqP7HYYkbbA1a5zvR5Lmm6Unrup3CJK03hyRIUmSJEmSBoaFDEmSJEmSNDAsZEiSJEmSpIEx5woZSY5M8sMkx/c7FkmSpPWR5L+TbN2WzWkkSRrDXJzs8xXAU6tqZb8DkSRJWh9V9bSel+Y0kiSNYU6NyEjyYeBBwElJ/j7J2Ukuat8f3rY5NMlXk3wzyU+SvLNn/5cm+XGS05J8NMkH+tUXSZI09yR5bZIj2/J7k3y3LT8pyWeTrEqyzaic5tVJNk/y8STnt9xm/372Q5KkfppThYyqOgK4CngC8CFgn6raFXgj8LaeTZcBBwGPAg5K8sAkDwD+EfgT4E+BHWcwdEmSND+cAezdloeALZJsAuwFnDmyUW9OU1XvBY4GvltVu9PlOe9KsvmMRi5J0iwxpwoZo2wFfCnJpcB7gZ161p1aVddX1e+Ay4EdgD2A06tqTVXdCnxpvAMnOTzJcJLhqpumsQuSJGmOuQDYLcmWwC3AD+gKGnvTU8gYw5OB1yVZDpwGbApsP9aGvXnK6tWrpzB0SZJmh7lcyHgr8L2q2hl4Jt0b/ohbepZvp5srJBM9cFUdV1VDVTWUbDYlwUqSpLmv3SxZBbwEOJuuePEE4MHAD9eya4BnV9Wy9rV9VY25fW+esnjx4qntgCRJs8BcLmRsBVzZlg+dwPbnAY9Pcp8kC4BnT1dgkiRpXjsDOKp9PxM4AlheVbWWfb4F/HWSACTZddqjlCRplprLhYx3Am9Pchaw8bo2rqor6ebROBf4Dt0jJ9dPa4SSJGk+OhPYFvhBVf0K+B1rf6wEupGmmwAr2mOzb53eECVJmr3m3MevVtWStngN8LCeVf/Y1n8S+GTP9s/o2eZzVXVcG5FxAnDKdMYqSZLmn6o6la4oMfL6YT3LS8ZZvhn4y5mJUJKk2W0uj8iYjDe3SbQuBVYCJ/Y1GkmSJEmSdDdzbkTGhqiqo/odgyRJkiRJGp8jMiRJkiRJ0sBwRMYGWrZsO4aHj+l3GJIkSdJ6W3nAkn6HIGkeyyT3c0SGJEmSJEkaGBYyJEmSJEnSwLCQIUmSJEmSBoZzZGyg5cuvZNGio/sdhiRtsDVrnO9HkgbV0hNX9TsESZoxjsiQJEmSJEkDw0KGJEmSJEkaGBYyJEmSJEnSwLCQMYYkWyd5Rb/jkCRJ80OSI5P8MMnx/Y5FkqTZzkLG2LYGLGRIkqSZ8grgaVV1SL8DkSRpthvYQkaSJUku7Xl9VJI3Jzktyb8mOTvJpUn2aOsfn2R5+7ooyZat/TVJzk+yIsk/tcO9A3hw2/ZdM987SZI0XyT5MPAg4KQkf99ymIva94e3bQ5N8tUk30zykyTv7G/UkiT1z1z9+NXNq+qxSfYBPg7sDBwF/FVVnZVkC+B3SZ4MPBTYAwhdArEP8Dpg56pa1p/wJUnSfFFVRyR5CvAE4PfAv1TVbUn2A94GPLttugzYFbgF+FGSf6uqX/QjZkmS+mmuFjI+D1BVZyRZmGRr4CzgPe3Z069W1S9bIePJwEVtvy3oChv/u7aDJzkcOBxgo40WTk8PJEnSfLQV8KkkDwUK2KRn3alVdT1AksuBHYB7FDJ685Ttt99+2gOWJGmmDeyjJcBt3D3+TXuWa9S2VVXvAP4CuDdwTpId6UZhvL2qlrWvh1TVf6zrxFV1XFUNVdVQstkGdkOSJOlObwW+V1U7A8/k7vnNLT3LtzPODanePGXx4sXTF6kkSX0yyIWMXwH3S3LfJPcCntGz7iCAJHsB11fV9UkeXFWXVNWxwDCwI/At4LD2qAlJtktyP+C3wJYz2RlJkiS6ERlXtuVD+xiHJEmz1sA+WlJVtyZ5C3AusBK4omf1tUnOBhYCh7W2VyV5At0djMuBb1TVLUkeAfwgCcANwAuq6qdJzmqTiX6jql4zQ92SJEnz2zvpHi35W+C7/Q5GkqTZKFWjn8IYbElOA46qquGZON+CBdvWwoWHrXtDSZrl1qw5pt8haBokuaCqhvodh/pjaGiohodnJCVSny09cVW/Q5Ck9bbqwKWTylMG+dESSZIkSZI0zwzsoyXjqap9+x2DJEmSJEmaHo7IkCRJkiRJA2POjciYacuWbcfwsM+VS5IkqX9WHrCk3yFI0nrLJPdzRIYkSZIkSRoYFjIkSZIkSdLAsJAhSZIkSZIGhnNkbKDly69k0aKj+x2GJG2wNWuc70eSZrulJ67qdwiS1HeOyJAkSZIkSQPDQoYkSZIkSRoYFjIkSZIkSdLAsJAhSZIkSZIGxkAWMpK8YQP3PyDJI3tefzLJczY8MkmSpLElWZLk0ik4zr5JHjsVMUmSNIgGspABbFAhAzgAeOS6NpIkSZqF9gUsZEiS5q1pLWQkeVGSFUkuTvKZJDskObW1nZpk+7bdJ5O8P8nZSX42MjoiybZJzkiyPMmlSfZO8g7g3q3t+LbdiUkuSHJZksN7zn9DkmPa+c9Jcv92B+NZwLvaMR7cs/2TkpzQ8/pPk3x1Oq+RJEmaVzZO8tGWs5yS5N5JHpzkmy2XOTPJjgBJnpnk3CQXJflOy2OWAEcAr255zN597Y0kSX0wbYWMJDsBRwNPrKpdgL8BPgB8uqoeDRwPvL9nl22BvYBnAO9obX8OfKuqlgG7AMur6nXAzVW1rKoOadsdVlW7AUPAkUnu29o3B85p5z8DeFlVnQ2cBLymHeOnPTF8F3hEksXt9UuAT4zRt8OTDCcZrrppchdIkiTNRw8F/r2qdgKuA54NHAf8dctljgI+2Lb9PvAnVbUr8AXgtVW1Cvgw8N6Wx5w5+gS9ecrq1aunvUOSJM20BdN47CcCX66qawCqak2SPYE/a+s/A7yzZ/sTq+oO4PIk929t5wMfT7JJW798nHMdmeTAtvxAuiTh/4DfAye39guAP11bwFVVST4DvCDJJ4A9gReNsd1xdEkHCxZsW2s7piRJUo+VPfnMBcASusdEvpRkZJt7te9/BHwxybbAHwArJ3KC3jxlaGjIPEWSNOdM56MlAdb15tm7/pZR+1JVZwD7AFcCn0lyj6JCkn2B/YA928iLi4BN2+pbq2rkHLczscLNJ4AXAAcDX6qq2yawjyRJ0kT05ju3A4uA69roipGvR7T1/wZ8oKoeBfwld+U3kiTNa9NZyDgVeN7IYx5JFgFnA89v6w+hGzI5riQ7AL+uqo8C/wE8pq26tY3SANgKuLaqbmrPlP7JBGL7LbDlWCuq6irgKuAfgE9O4FiSJEmT9RtgZZLnAqSzS1u3Fd3NHIAX9+wzbh4jSdJ8MG2FjKq6DDgGOD3JxcB7gCOBlyRZAbyQbt6MtdkXWJ7kIrpnSN/X2o8DVrTJPr8JLGjHfCtwzgTC+wLwmjZ51oPHWH888IuqunwCx5IkSdoQhwAvbfnSZcD+rf3NdI+cnAlc07P9fwEHOtmnJGm+yl1PXmhEkg8AF1XVf6xr2wULtq2FCw+bgagkaXqtWXNMv0PQNEhyQVUN9TsO9cfQ0FANDw/3OwxNoaUnrup3CJI0ZVYduHRSecp0TvY5kJJcANwI/F2/Y5EkSZIkSXdnIWOU9tFnkiRJkiRpFrKQsYGWLduO4WGHY0uSJGn6rTxgSb9DkKQpk3VvMqbp/NQSSZIkSZKkKWUhQ5IkSZIkDQwLGZIkSZIkaWA4R8YGWr78ShYtOrrfYUjSBvPjVyVp9vBjViVpfI7IkCRJkiRJA8NChiRJkiRJGhgWMiRJkiRJ0sCwkCFJkiRJkgbGwBcykixJcukY7aclGZrE8d6c5KipiU6SJGnykrwlyX5jtO+b5OR+xCRJUr/5qSWSJEmzVFW9sd8xSJI02wz8iIxmQZJPJVmR5MtJNutdmeTgJJckuTTJsT3tT0lyYZKLk5w6+qBJXpbkG0nuPROdkCRJc0uSF7X85OIkn0myQ5JTW9upSbZPslWSVUk2avtsluQXSTZJ8skkz2ntT0lyRZLvA3/W145JktRHc6WQ8XDguKp6NPAb4BUjK5I8ADgWeCKwDNg9yQFJFgMfBZ5dVbsAz+09YJJXAs8EDqiqm0etOzzJcJLhqpumsVuSJGlQJdkJOBp4Yss1/gb4APDplrMcD7y/qq4HLgYe33Z9JvCtqrq151ib0uUtzwT2Bv5wLee9M09ZvXr1NPRMkqT+miuFjF9U1Vlt+bPAXj3rdgdOq6rVVXUbXdKwD/AnwBlVtRKgqtb07PNC4Kl0RY5bRp+sqo6rqqGqGho1+EOSJGnEE4EvV9U1cGeusSfwubb+M9yVs3wROKgtP7+97rUjsLKqflJVRZfvjKk3T1m8ePHU9ESSpFlkrhQyai2vM84+GWO/EZcCS4A/2rCwJEnSPLa2XGPEyPqTgKcmWQTsBnx3LdtKkjSvzZVCxvZJ9mzLBwPf71l3LvD4JNsk2bitPx34QWtfCtAShxEXAX8JnNQeTZEkSVpfpwLPS3JfuDPXOJtuxAXAIbScpapuAM4D3gecXFW3jzrWFcDSJA9urw+e5tglSZq15koh44fAi5OsABYBHxpZUVVXA68Hvkf3/OmFVfW1qloNHA58NcnFjBrCWVXfB44Cvp5km5nphiRJmiuq6jLgGOD0lmu8BzgSeEnLWV5IN2/GiC8CL+Cej5VQVb+jy1u+3ib7/Pk0hy9J0qyV7jFLTdaCBdvWwoWH9TsMSdpga9Yc0+8QNA2SXFBVQ/2OQ/0xNDRUw8PD/Q5Dk7D0xFX9DkGSpt2qA5dOKk+ZKyMyJEmSJEnSPGAhQ5IkSZIkDYwF/Q5g0C1bth3Dww7HliRJ0tRZecCSfocgSdNuvI8YXRdHZEiSJEmSpIFhIUOSJEmSJA0MCxmSJEmSJGlgOEfGBlq+/EoWLTq632FI0gbz41claXx+HKokzR6OyJAkSZIkSQPDQoYkSZIkSRoYFjIkSZIkSdLAsJAhSZIkSZIGxqwvZCR5c5KjkrwlyX6tbe8klyVZnuTeSd7VXr8ryRFJXjTJc52WZKgtv2Eq+yFJkgbbRHKMJIcm+cA4626YnsgkSZpfBuZTS6rqjT0vDwHeXVWfAEjyl8DiqrplCk/5BuBtU3g8SZI0wKrqw/2OYbQkC6rqtn7HIUnSTJqVIzKSHJ3kR0m+Azy8tX0yyXOS/AXwPOCNSY5PchKwOXBukoNGRnC0fR6S5DtJLk5yYZIHJ9k3yck95/pAkkNHnf8dwL3biI/jZ6jbkiRpBiVZkuSHST7aRnae0kZ6PjjJN5NckOTMJDu27XtzjN2TrEjygzYi9NKeQz+g7f+TJO8cdc5/aTnJqUkWt7ZlSc5pxzshyX1ae+9I0W2SrGrLhyb5UpL/Ak6Z9gslSdIsM+sKGUl2A54P7Ar8GbB77/qq+hhwEvCaqjqkqp4F3FxVy6rqi6MOdzzw71W1C/BY4OqJxFBVr+s55iFjxHh4kuEkw1U3rW8XJUnS7PFQulxhJ+A64NnAccBfV9VuwFHAB8fY7xPAEVW1J3D7qHXLgIOARwEHJXlga98cuLCqHgOcDryptX8a+PuqejRwSU/72uwJvLiqnjh6RW+esnr16gkcSpKkwTLrChnA3sAJVXVTVf2Grmix3pJsCWxXVScAVNXvaoqqDlV1XFUNVdVQstlUHFKSJPXHyqpa3pYvAJbQ3fz4UpLlwEeAbXt3SLI1sGVVnd2aPjfqmKdW1fVV9TvgcmCH1n4HMHLT5bPAXkm2ArauqtNb+6eAfSYQ97eras1YK3rzlMWLF0/gUJIkDZbZOkdGTcExMk77bdy9gLPpFJxLkiQNpt75tW4H7g9cV1XL1rLPeDnGeMccL99aV77Tm7OMzlduXMe+kiTNWbNxRMYZwIHtGdUtgWdO5iBtNMcvkxwAkORe6YZP/Bx4ZHu9FfCkcQ5xa5JNJnNuSZI0sH4DrEzyXIB0dundoKquBX6b5E9a0/MneOyNgOe05T8Hvl9V1wPXJtm7tb+Q7rETgFXAbm15ZD9Jkua9WVfIqKoL6YZdLge+Apy5AYd7IXBkkhXA2cAfVtUvgP8EVtDNoXHROPseB6xwsk9JkuadQ4CXJrkYuAzYf4xtXgocl+QHdCM0rp/AcW8EdkpyAfBE4C2t/cXAu1q+sqyn/d3Ay5OcDWwzyb5IkjTnpGoqnuKYvxYs2LYWLjys32FI0gZbs+aYfoegaZDkgqoa6nccc02SLarqhrb8OmDbqvqbPod1D0NDQzU8PNzvMOaEpSeu6ncIkjTnrDpw6aTylNk6R4YkSdJs9vQkr6fLpX4OHNrfcCRJmj8sZEiSJK2n9pHvoz/2XZIkzQALGRto2bLtGB52OLYkSdJctvKAJf0OQZLmnHV9DNh4Zt1kn5IkSZIkSeOxkCFJkiRJkgaGhQxJkiRJkjQwnCNjAy1ffiWLFh3d7zAkaYP58auS1PGjViVpdnNEhiRJkiRJGhgWMiRJkiRJ0sCwkCFJkiRJkgaGhQxJkiRJkjQw5k0hI8mRSX6Y5Phx1g8lef9MxyVJkpTk7H7HIEnSoJhPn1ryCuCpVbVyrJVVNQwMj25PsqCqbpvu4CRJ0vxVVY/tdwySJA2KeTEiI8mHgQcBJyX5+yRnJ7mofX9422bfJCe35TcnOS7JKcCn+xi6JEmaJZIcm+QVPa/fnORNSU5NcmGSS5Ls37P+RUlWJLk4yWda2/2TnNDaLk7y2NZ+Q/u+b5LTknw5yRVJjk+Stm63JKcnuSDJt5JsO7NXQJKk2WFejMioqiOSPAV4AvB74F+q6rYk+wFvA549xm67AXtV1c2jVyQ5HDgcYKONFk5f4JIkaTb5AvCvwAfb6+cBTwHeW1W/SbINcE6Sk4BHAkcDj6uqa5Isavu8Hzi9qg5MsjGwxRjn2RXYCbgKOAt4XJJzgX8D9q+q1UkOAo4BDhu9c2+esv32209BtyVJml3mRSFjlK2ATyV5KFDAJuNsd9JYRQyAqjoOOA5gwYJta1qilCRJs0pVXZTkfkkeACwGrgWuBt6bZB/gDmA74P7AE4EvV9U1bd817TBPBF7U2m4Hrh/jVOdV1S8BkiwHlgDXATsD324DNDZu5x4rzjvzlKGhIfMUSdKcMx8LGW8FvtfuhCwBThtnuxtnLCJJkjQovgw8B/hDuhEah9AVNXarqluTrAI2BUJ3w2QybulZvp0uXwtwWVXtOcljSpI0Z8yLOTJG2Qq4si0f2sc4JEnS4PkC8Hy6YsaX6fKKX7cixhOAHdp2pwLPS3JfgJ5HS04FXt7aNk4y0WdUfwQsTrJn23eTJDtNRYckSRo087GQ8U7g7UnOohuWKUmSNCFVdRmwJXBlVV0NHA8MJRmmG51xRc92xwCnJ7kYeE87xN8AT0hyCXAB3VwYEznv7+mKJ8e24y0H/KQTSdK8lCofndwQCxZsWwsX3mOeLUkaOGvWHNPvEDQNklxQVUP9jkP9MTQ0VMPD9/h0ea3D0hNX9TsESZoXVh24dFJ5ynwckSFJkiRJkgaUhQxJkiRJkjQw5uOnlkypZcu2Y3jY4diSJElzxcoDlvQ7BEmaFzLJ/RyRIUmSJEmSBoaFDEmSJEmSNDAsZEiSJEmSpIHhHBkbaPnyK1m06Oh+hyFJG8yPX5U0KPx4VEma3xyRIUmSJEmSBoaFDEmSJEmSNDAsZEiSJEmSpIFhIUOSJEmSJA0MCxmSJEkDIMlpSYb6HYckSf1mIaNJx+shSZIkSdIs1tc/3JOcmOSCJJclOby13ZDkmCQXJzknyf2TbJlkZZJN2jYLk6xKskm7O/HeJGck+WGS3ZN8NclPkvxzz7n+Nsml7etVrW1J2+eDwIXAA5O8Jsn5SVYk+ac+XBZJkjSP9eQnH2050ilJ7t1WPzfJeUl+nGTvvgYqSVKf9HsEwmFVtRswBByZ5L7A5sA5VbULcAbwsqr6LXAa8PS23/OBr1TVre3176tqH+DDwNeAvwJ2Bg5Nct8kuwEvAf4Y+BPgZUl2bfs+HPh0Ve3alh8K7AEsA3ZLss+09V6SJGlsDwX+vap2Aq4Dnt3aF1TVHsCrgDf1JzRJkvqr34WMI5NcDJwDPJDuTfv3wMlt/QXAkrb8MbpiBO37J3qOc1L7fglwWVVdXVW3AD9rx90LOKGqbqyqG4CvAiN3MX5eVee05Se3r4voRmjs2GK6mySHJxlOMlx102T7LkmSNJ6VVbW8LffmQ18do+1uevOU1atXT2eMkiT1xYJ+nTjJvsB+wJ5VdVOS04BNgVurqtpmt9NirKqz2lDLxwMbV9WlPYe7pX2/o2d55PUCIGsJ5cbesIC3V9VH1hZ7VR0HHAewYMG2tbZtJUmSJqE3n7kduPeo9jtzpNF685ShoSHzFEnSnNPPERlbAde2IsaOdI98rMungc9z99EYE3EGcECSzZJsDhwInDnGdt8CDkuyBUCS7ZLcbz3PJUmSJEmSpkk/CxnfBBYkWQG8le7xknU5HrgPXTFjwqrqQuCTwHnAucDHquqiMbY7Bfgc8IMklwBfBrZcn3NJkiRJkqTpk7ue4pj9kjwH2L+qXtjvWEYsWLBtLVx4WL/DkKQNtmbNMf0OQdMgyQVVNdTvONQfQ0NDNTw83O8wptzSE1f1OwRJ0hRYdeDSSeUpfZsjY30l+TfgqcDT+h2LJEmSJEnqj4EpZFTVX/c7BkmSJEmS1F/9/vhVSZIkSZKkCRuYERmz1bJl2zE87HPlkiRJM2XlAUv6HYIkaQpkkvs5IkOSJEmSJA0MCxmSJEmSJGlgWMiQJEmSJEkDwzkyNtDy5VeyaNHR/Q5DkjbYmjXO9yNp7ZaeuKrfIUiS5IgMSZIkSZI0OCxkSJIkSZKkgWEhQ5IkSZIkDYw5X8hIsiTJpeux/ceSPLItr0qyzfRFJ0mSNDHrm9NIkjRXOdnnKFX1F/2OQZIkSZIkjW3Oj8hoFiT5VJIVSb6cZLMkT0pyUZJLknw8yb0AkpyWZKjfAUuSpMHWRlBc0UZ7Xprk+CT7JTkryU+S7NG+zm45ydlJHt723SnJeUmWt/zloaOO/aC2z+796Z0kSf0zXwoZDweOq6pHA78B/hb4JHBQVT2KbmTKy/sXniRJmqMeArwPeDSwI/DnwF7AUcAbgCuAfapqV+CNwNvafkcA76uqZcAQ8MuRA7Zix1eAl1TV+TPTDUmSZo/5Usj4RVWd1ZY/CzwJWFlVP25tnwL2mejBkhyeZDjJcNVNUxyqJEmaQ1ZW1SVVdQdwGXBqVRVwCbAE2Ar4Upv74r3ATm2/HwBvSPL3wA5VdXNrXwx8DXhBVS0f64S9ecrq1aunq1+SJPXNfClk1JQerOq4qhqqqqFks6k8tCRJmltu6Vm+o+f1HXQjQt8KfK+qdgaeCWwKUFWfA54F3Ax8K8kT237XA78AHjfeCXvzlMWLF09lXyRJmhXmSyFj+yR7tuWDge8AS5I8pLW9EDi9L5FJkqT5bCvgyrZ86EhjkgcBP6uq9wMn0T2aAvB74ADgRUn+fObClCRp9pgvhYwfAi9OsgJYRDd08yV0Qzkvobsr8uE+xidJkuandwJvT3IWsHFP+0HApUmW082t8emRFVV1I/AM4NVJ9p/BWCVJmhXm/MevVtUq4JFjrDoV2HWM7fftWV4yXXFJkqS5reUgO/e8PnScdQ/r2e0f2/q3A28fdcg1I/tU1XWAn1giSZqX5suIDEmSJEmSNAdYyJAkSZIkSQPDQoYkSZIkSRoYc36OjOm2bNl2DA8f0+8wJEmSpt3KA5b0OwRJ0hySSe7niAxJkiRJkjQwUlX9jmGgJfkt8KN+xzFFtgGu6XcQU8B+zD5zpS/2Y/aZK32Zzn7sUFWLp+nYmuWSrAZ+3u84mrny+zrTvG6T43WbHK/b5HntJufhVbXl+u7koyUb7kdVNdTvIKZCkuG50Bf7MfvMlb7Yj9lnrvRlrvRDs89sKmL5cz45XrfJ8bpNjtdt8rx2k5NkeDL7+WiJJEmSJEkaGBYyJEmSJEnSwLCQseGO63cAU2iu9MV+zD5zpS/2Y/aZK32ZK/2Q1saf88nxuk2O121yvG6T57WbnEldNyf7lCRJkiRJA8MRGZIkSZIkaWBYyFiLJE9J8qMk/5PkdWOsT5L3t/UrkjxmovvOpAn045AW/4okZyfZpWfdqiSXJFk+2Rllp8oE+rFvkutbrMuTvHGi+860CfTlNT39uDTJ7UkWtXWz6d/k40l+neTScdYPyu/IuvoxKL8j6+rHIP2OrKsvs/53JMkDk3wvyQ+TXJbkb8bYZiB+R6TJSPKuJFe0n+0Tkmw9znaz4nd2tliP6+b/ET2SPLf9X3tHknE/OcKft7tbj+vmz1uPJIuSfDvJT9r3+4yznT9vbNjf1eOqKr/G+AI2Bn4KPAj4A+Bi4JGjtnka8A0gwJ8A505031nWj8cC92nLTx3pR3u9CthmQP499gVOnsy+s60vo7Z/JvDd2fZv0mLZB3gMcOk462f978gE+zHrf0cm2I+B+B2ZSF9GbTsrf0eAbYHHtOUtgR8P4vuIX35N9gt4MrCgLR8LHDvOdrPid3a2fE3kuvl/xJjX7RHAw4HTgKG1bOfP23peN3/exrwm7wRe15Zf5/9va71Wk/67em1fjsgY3x7A/1TVz6rq98AXgP1HbbM/8OnqnANsnWTbCe47U9YZS1WdXVXXtpfnAH80wzFOxIZc09n07zGZeA4GPj8jka2nqjoDWLOWTQbhd2Sd/RiQ35GJ/HuMZ1b9e8B692VW/o5U1dVVdWFb/i3wQ2C7UZsNxO+INBlVdUpV3dZeztr/O2ebCV43/48Ypap+WFU/6nccg2aC182ft3vaH/hUW/4UcED/Qpn1NuTv6nFZyBjfdsAvel7/knsmoONtM5F9Z8r6xvJSumrYiAJOSXJBksOnIb6Jmmg/9kxycZJvJNlpPfedKROOJ8lmwFOAr/Q0z5Z/k4kYhN+R9TVbf0cmahB+RyZsUH5HkiwBdgXOHbVqLv6OSGM5jLv/39lr1v3OziLjXTf/j5g8f97Wnz9v93T/qroauhsXwP3G2c6ftw37u3pcC6YktLkpY7SN/oiX8baZyL4zZcKxJHkC3R9pe/U0P66qrkpyP+DbSa5od0pn2kT6cSGwQ1XdkORpwInAQye470xan3ieCZxVVb13pmfLv8lEDMLvyITN8t+RiRiU35H1Met/R5JsQVdoeVVV/Wb06jF2GdjfEc0/Sb4D/OEYq46uqq+1bY4GbgOOH+cws+p3diZMwXWbl/9HTOS6TYA/b3eZ6HXz5+3ujl6Pw8y7n7cxbMjf1eOykDG+XwIP7Hn9R8BVE9zmDyaw70yZSD9I8mjgY8BTq+r/Rtqr6qr2/ddJTqAbGtSPX7519qP3D4Sq+u8kH0yyzUT2nWHrE8/zGTVkfhb9m0zEIPyOTMgA/I6s0wD9jqyPWf07kmQTuiLG8VX11TE2mTO/I5qfqmq/ta1P8mLgGcCTqj0IPcYxZs3v7EyZgus2yP9vT9q6rtsEj+HP2/rz522UJL9Ksm1VXd0egfj1OMeYdz9vY9iQv6vH5aMl4zsfeGiSpUn+gC5ZPmnUNicBL2qzrP4JcH0bWjSRfWfKOmNJsj3wVeCFVfXjnvbNk2w5skw3+dSYnyAwAybSjz9Mkra8B93P9/9NZN8ZNqF4kmwFPB74Wk/bbPo3mYhB+B1ZpwH5HVmnAfodmZDZ/jvSrvV/AD+sqveMs9mc+B2RxpLkKcDfA8+qqpvG2WbW/M7OFhO5bvh/xKT48zZp/rzd00nAi9vyi+nJRUb483anDfm7elyOyBhHVd2W5JXAt+hmWv14VV2W5Ii2/sPAf9PNsPo/wE3AS9a2bx+6MdF+vBG4L/DB9jfObVU1BNwfOKG1LQA+V1Xf7EM3JtqP5wAvT3IbcDPw/HYXY9b8e7RYJ9IXgAOBU6rqxp7dZ82/CUCSz9N9EsY2SX4JvAnYBAbndwQm1I9Z/zsCE+rHQPyOwIT6ArP/d+RxwAuBS5Isb21vALaHwfodkSbpA8C96IZTA5xTVUckeQDwsap6GrPrd3a2WOd18/+Ie0pyIPBvwGLg60mWV9X/58/b2k3kuvnzNqZ3AP+Z5KXA/wLPBfDn7Z425O/qtck4o/wkSZIkSZJmHR8tkSRJkiRJA8NChiRJkiRJGhgWMiRJkiRJ0sCwkCFJkiRJkgaGhQxJkiRJkjQwLGRIuockN0zx8ZYkuTnJ8iSXJ/lwkgn//5PkzUmOmsB2t7dzXJbk4iR/O3KeJENJ3t+W75XkO23bg5Ls3fZZnuTek++pJEmabuYpkhb0OwBJ88ZPq2pZkgXAd4EDgK+OrEyyoKpu28Bz3FxVy9rx7gd8DtgKeFNVDQPDbbtdgU16tv0w8O6q+sQGnl+SJA0m8xRpgDgiQ9KEJFmW5JwkK5KckOQ+rX331vaDJO9KcunajtOSgLOBhyQ5NMmXkvwXcEqSRUlObMc7J8mje3bdJcl3k/wkycvWFW9V/Ro4HHhlOvsmObklDp8FlrU7G38JPA94Y5LjJ3d1JElSP5mnSPOLhQxJE/Vp4O+r6tHAJcCbWvsngCOqak/g9nUdJMlmwJPaMQD2BF5cVU8E/gm4qJ3jDe2cIx4NPL1t/8YkD1jXuarqZ3T/z92vp+3XwF8AZ1bVsqr6CHAS8JqqOmRdx5QkSbOSeYo0j1jIkLROSbYCtq6q01vTp4B9kmwNbFlVZ7f2z63lMA9Oshw4C/h6VX2jtX+7qta05b2AzwBU1XeB+7ZzA3ytqm6uqmuA7wF7TDT8CW4nSZIGkHmKNP84R4akDbE+b74/HXnWc5Qb13G8GvV9dPu4kjyI7u7Lr4FHTCBGSZI0d5inSHOUIzIkrVNVXQ9cm2Tv1vRC4PSquhb4bZI/ae3P38BTnQEcApBkX+CaqvpNW7d/kk2T3BfYFzh/bQdKshj4MPCBqlpnMiFJkgaTeYo0/zgiQ9JYNkvyy57X7wFeDHy4PTv6M+Albd1LgY8muRE4Dbh+A877ZuATSVYAN7VzjjgP+DqwPfDWqrpqjP3v3YaFbgLcRjf88z0bEI8kSZp9zFOkeS4WACVtiCRbVNUNbfl1wLZV9Td9DkuSJMk8RZqjHJEhaUM9Pcnr6f4/+TlwaH/DkSRJupN5ijQHOSJDkiRJkiQNDCf7lCRJkiRJA8NChiRJkiRJGhgWMiRJkiRJ0sCwkCFJkiRJkgaGhQxJkiRJkjQwLGRIkiRJkqSB8f8DnPHE5Pu6Zd0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1224x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(17,7))\n",
    "\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "sns.barplot(data=log_diff_df.sort_values(ascending=False, by='Log Diff').head(20), y='Word', x='Log Diff', color='Darkblue', ax=ax1)\n",
    "ax1.set_title('Problems: Most Dominant Words')\n",
    "ax1.set_xlabel('Log Prob Diff')\n",
    "\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "sns.barplot(data=log_diff_df.sort_values(ascending=False, by='Log Diff').tail(20), y='Word', x='Log Diff', color='deepskyblue', ax=ax2)\n",
    "ax2.set_title('Success: Most Dominant Words')\n",
    "ax2.set_xlabel('Log Prob Diff');\n",
    "\n",
    "plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.3, hspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ca02f4",
   "metadata": {},
   "source": [
    "### Misclassification Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d9b9c5",
   "metadata": {},
   "source": [
    "The model is not perfect and has gone wrong in some predictions. Before looking at the misclassified posts, I plotted the ROC curve which reflected an AUC score of 0.85 for my model. Coincidentally, we have quite similar false positive (72 posts) and false negatives (68 posts) - this is fairly ideal since we are aiming to be impartial to either false postives or negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9475a226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc_curve(gs_cvnb, X_test, y_test, name='CVEC M. Naive Bayes')\n",
    "plt.plot([0, 1], [0, 1],\n",
    "         label='baseline', linestyle='--')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "905c3e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8531324980005333"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ROC AUC score\n",
    "\n",
    "roc_auc_score(y_test, gs_cvnb.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e0adc0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_cvnb = gs_cvnb.predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, preds_cvnb).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "afa3b99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(gs_cvnb, X_test, y_test, cmap='Blues', values_format='d');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a6bbea",
   "metadata": {},
   "source": [
    "In the two tables below, I have reflected the 'worst' misclassified posts by sub-reddits. This definition of 'worst' is based on the model having a very high predict_proba for a particular class, and thus getting it very wrong when it misclassifies the post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797cab30",
   "metadata": {},
   "source": [
    "I have selected some of these posts to explain why the model could have misclassified them:\n",
    "\n",
    "**Sarcasm**<br>\n",
    "*'getting mildly excited buying a new vacuum cleaner head and realising you’ve reached peak middle age at 34.'* \n",
    "\n",
    "- This is wrongly classified as 'Success'.\n",
    "- This is a classic example of self-depracating British humour in the form of sarcasm. Words associated with positivity ('excited', 'new', 'peak') probably led to this being misclassified as a 'Success'. \n",
    "- To improve, new models to perform sentence embedding (e.g. BERT) would potentially perform better. Instead of splitting up posts by words, and in the process losing their context, sentence embedding can might be useful in analysing the relationship between set of words/sentences in a more dynamic manner.\n",
    "\n",
    "**Irony/Punchline at the end**<br>\n",
    "while in a queue of car going the speed limit in a 30, i got overtaken by a numpty in a black bmw hatchback going around 50. i watched him dangerously weave through traffic while not progressing very far... he rev up, go nowhere and a plume of smoke come out of the back... i don't usually enjoy the mishap of others, but this really cheered up my day! i watched a boy racer in a bmw fry his engine\n",
    "\n",
    "- This is wrongly classified as 'Problem'.\n",
    "- The bulk of this post was to desribed the bad bahviour of a fellow motorist and plenty of negative descriptions were used ('dangerously', 'nowhere', 'crash', 'idiot'). However, the punchline of the joke comes in the end when the poster realised that the motorist had crashed and that 'really cheered up my day!'.\n",
    "- Likewise, sentence embedding would potentially help improve the misclassificatioin of such examples.\n",
    "\n",
    "\n",
    "***Coincidence***<br>\n",
    "*'it get hoovered several time a week. is this some late-stage capitalism insanity, or am i just out of the loop? the sun is finally out after a wet saturday morning, perfect to enjoy some time in the garden, to the maddening soundtrack of next door hoovering their artificial grass.'*\n",
    "\n",
    "- This is wrongly classified as 'Success'. There could be a few factors such as the positive words used in the posts ('perfect', 'enjoy').\n",
    "- However, my suspicion is on the word 'Saturday' - this was one of the most 'different' word to explain 'British Success'. Upon further investigation, I found out the dataset contained 11 mentions of 'Saturday' in 'British Success' vs 4 mentions of in 'British Problems', and **all the mentions of 'Saturday' in the test set are for 'British Problems'**.\n",
    "- This is an issue because out model would have likely to have trained on 'Saturday' in 'British Succes' posts only. When 'Saturday' appeared in 'British Problems' as per the test dataset - there would be a fair chance that this gets misclassified.\n",
    "- A larger dataset would help avoid this problem as words will appear more frequently and have a high chance of being shuffled across both train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8158b7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "problems_prob = [problems[1] for problems in gs_cvnb.predict_proba(X_test)]\n",
    "success_prob = [success[0] for success in gs_cvnb.predict_proba(X_test)]\n",
    "prediction = gs_cvnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bcbe0511",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob = list(zip(success_prob, problems_prob, prediction, y_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "891af606",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob_df = pd.DataFrame(pred_prob, columns=['Success','Problems','Pred Value','True Value','Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b2c54f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_predictions = pred_prob_df[pred_prob_df['Pred Value'] != pred_prob_df['True Value']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80444b4c",
   "metadata": {},
   "source": [
    "**Misclassified: Predict = British Problems but True = British Success**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ff37a1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Success</th>\n",
       "      <th>Problems</th>\n",
       "      <th>Pred Value</th>\n",
       "      <th>True Value</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>0.001755</td>\n",
       "      <td>0.998245</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>while in a queue of car going the speed limit in a 30, i got overtaken by a numpty in a black bmw hatchback going around 50. i watched him dangerously weave through traffic while not progressing very far, and ended up sat behind him at a roundabout leading to a quiet industrial estate road. i watched him floor it out of sight, silently hoping that he crash into a tree rather than someone else on the road. a i got towards the end of the road, i see he's made a u turn and is about to do a return sprint. he rev up, go nowhere and a plume of smoke come out of the back. shortly after the hazard light come on and in my mirror i see the idiot getting out of his broken car. i don't usually enjoy the mishap of others, but this really cheered up my day! i watched a boy racer in a bmw fry his engine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>0.002541</td>\n",
       "      <td>0.997459</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>a everyone else turn their nose up at anything vegan, and i am notoriously not fussy over what i'm consuming, he is happily sharing his stash with me. bonus: he doesn't care that what i get from the oops bay is rarely veggie, never mind vegan, and doesn't ask me to eat at a separate table from him. one of my colleague this week is a vegan and a sharer of snack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0.003060</td>\n",
       "      <td>0.996940</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>nothing worse than sitting on a bus in 17c weather waiting for the driver to change. the bus drove straight past the depot without changing driver both ways.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.005404</td>\n",
       "      <td>0.994596</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>love how #craftywank is a thing. i mean how doe one explain it to anyone other than a brit. it’s so ridiculous but so funny. faith in twitter restored</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.006707</td>\n",
       "      <td>0.993293</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>danny, who review takeaway on yt. he's fantastic, he's a lovely bloke, he's exploding. no, i'm not related to him. get in now so you can say \"oh yeah, i've been watching him for ages\" to your mates.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>0.006967</td>\n",
       "      <td>0.993033</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>yeah, i probably sound like a bastard 'dobbing' someone in, but drive like a dick then take a shortcut down a bus route... you're a twat! submitting dash cam footage to the police, and the police taking action!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>0.012116</td>\n",
       "      <td>0.987884</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>always risky to insult someone's work when they're holding a bladed weapon while standing right behind you but this time it paid off found the courage to tell the hairdresser i didn't like my hair at the end of the haircut and she wa fine with it and fixed it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0.012383</td>\n",
       "      <td>0.987617</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>i've put this a a success rather than a problem because i find their outlook so refreshing and it make me smile that they see people a good decade younger than them a \"old\". my 84 and 87 year old grandparent complaining about the grey hair/blue rinse brigade at the new working men's club they've joined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>0.016468</td>\n",
       "      <td>0.983532</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>i just thought 'this is a good thing'. year ago there would have been zillion flying around. i live in a house backing on to fields, so i shouldn't be short of them. now i'm welcoming them, thinking there should be more. i certainly don't want to swat them any more. not getting bugged by the number of bug flying around my room with it window open.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.021528</td>\n",
       "      <td>0.978472</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>using your jet wash to turn off next door loud music system in the garden. instant justice these idiot need to buy some wireless headphones.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Success  Problems  Pred Value  True Value  \\\n",
       "640  0.001755  0.998245  1           0            \n",
       "265  0.002541  0.997459  1           0            \n",
       "235  0.003060  0.996940  1           0            \n",
       "102  0.005404  0.994596  1           0            \n",
       "131  0.006707  0.993293  1           0            \n",
       "356  0.006967  0.993033  1           0            \n",
       "589  0.012116  0.987884  1           0            \n",
       "248  0.012383  0.987617  1           0            \n",
       "569  0.016468  0.983532  1           0            \n",
       "7    0.021528  0.978472  1           0            \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Content  \n",
       "640  while in a queue of car going the speed limit in a 30, i got overtaken by a numpty in a black bmw hatchback going around 50. i watched him dangerously weave through traffic while not progressing very far, and ended up sat behind him at a roundabout leading to a quiet industrial estate road. i watched him floor it out of sight, silently hoping that he crash into a tree rather than someone else on the road. a i got towards the end of the road, i see he's made a u turn and is about to do a return sprint. he rev up, go nowhere and a plume of smoke come out of the back. shortly after the hazard light come on and in my mirror i see the idiot getting out of his broken car. i don't usually enjoy the mishap of others, but this really cheered up my day! i watched a boy racer in a bmw fry his engine  \n",
       "265  a everyone else turn their nose up at anything vegan, and i am notoriously not fussy over what i'm consuming, he is happily sharing his stash with me. bonus: he doesn't care that what i get from the oops bay is rarely veggie, never mind vegan, and doesn't ask me to eat at a separate table from him. one of my colleague this week is a vegan and a sharer of snack                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "235  nothing worse than sitting on a bus in 17c weather waiting for the driver to change. the bus drove straight past the depot without changing driver both ways.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "102  love how #craftywank is a thing. i mean how doe one explain it to anyone other than a brit. it’s so ridiculous but so funny. faith in twitter restored                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "131  danny, who review takeaway on yt. he's fantastic, he's a lovely bloke, he's exploding. no, i'm not related to him. get in now so you can say \"oh yeah, i've been watching him for ages\" to your mates.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "356  yeah, i probably sound like a bastard 'dobbing' someone in, but drive like a dick then take a shortcut down a bus route... you're a twat! submitting dash cam footage to the police, and the police taking action!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "589  always risky to insult someone's work when they're holding a bladed weapon while standing right behind you but this time it paid off found the courage to tell the hairdresser i didn't like my hair at the end of the haircut and she wa fine with it and fixed it                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "248  i've put this a a success rather than a problem because i find their outlook so refreshing and it make me smile that they see people a good decade younger than them a \"old\". my 84 and 87 year old grandparent complaining about the grey hair/blue rinse brigade at the new working men's club they've joined                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "569  i just thought 'this is a good thing'. year ago there would have been zillion flying around. i live in a house backing on to fields, so i shouldn't be short of them. now i'm welcoming them, thinking there should be more. i certainly don't want to swat them any more. not getting bugged by the number of bug flying around my room with it window open.                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "7    using your jet wash to turn off next door loud music system in the garden. instant justice these idiot need to buy some wireless headphones.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_predictions.sort_values(by='Problems', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b26950e",
   "metadata": {},
   "source": [
    "**Misclassified: Predict = British Success but True = British Problems**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9b15a0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Success</th>\n",
       "      <th>Problems</th>\n",
       "      <th>Pred Value</th>\n",
       "      <th>True Value</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.414174e-10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>we bought our first house together last month. the wife hated the carpet and wall in the main bedroom but our work holiday diary were full. so for the pas two weeks, i've ripped up the old carpet and the laminate flooring below it, got an electrician to put in three double plug socket with usb point a there wa just a single plug socket in the whole room, repainted every wall white with four coat (they were all pink before). and reprinted the shirting-boards. all mainly on my hour lunch break while i'm working from home. we also needed a plasticer to fill in the hole left from the new plug sockets. we couldn't find a qualified plasticer so the wife got some training at work from a plasticer (she work on a building site.) and she did at the plasticing a couple of weekend ago. we were just replacing the curtain pole when the wife spotted a leak. she wa just out the shower. it look like there is a leak in the en-suite shower room. i did get the week off this week so i can decorate the dinner room, hallway and living room with the carpet fixer coming monday morning 9am to fix the new carpet in the bedroom. we, just, managed to get hold of the carpet fitter to postpone it a few week and the insurance company. both closed at 4pm on a sunday. we discovered the leak at 345pm. now a plumber coming instead of carpet fitter and i'm going to have to re-decorate half of that bedroom again once it's all be sorted. better go to b&amp;amp;q for more paint. spending lunchtime for the past two week (and two weekend in a row) decorating the main bedroom. finishing it on the sunday afternoon before the carpet fitter come monday... just found a leak!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>0.999975</td>\n",
       "      <td>2.498114e-05</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>it get hoovered several time a week. is this some late-stage capitalism insanity, or am i just out of the loop? the sun is finally out after a wet saturday morning, perfect to enjoy some time in the garden, to the maddening soundtrack of next door hoovering their artificial grass.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>0.999623</td>\n",
       "      <td>3.765296e-04</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>for my sims i watch a certain breakfast to show, and it seems like near enough every week there is a “these award ceremony went on last night, let’s look at who won, what they won, what they won it for, what they wore, the party they went to, what they said, etc” &amp;amp;#x200b; i really don’t care, i will watch your tv programmes, but couldn’t give a crap about the award you then win, or who should have won instead not caring one bit about “award shows”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>0.998978</td>\n",
       "      <td>1.021585e-03</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>there wa one small piece of sausage in the whole slice! one! would not have again saw a post yesterday about vegan greggs product with great reviews, thought i'd try the vegan sausage cheesz and bean slice - wa massively disappointed!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>0.995229</td>\n",
       "      <td>4.770634e-03</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>the guy picked it up with a bag, but the smell still reached me at the other end of the carriage. a dog just took a shit on the floor of the overground. happy monday morning..!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>0.995215</td>\n",
       "      <td>4.785398e-03</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>unfortunately waterstones ship their book in cardboard so i now have a signed first edition of a book no longer available to buy with run ink and damaged page getting home after a week to find that, instead of taking an undelivered item to the sorting office, the postman decided to drop the clearly labelled waterstones package over my back fence for it to sit in the rain in the flower bed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>0.993099</td>\n",
       "      <td>6.901117e-03</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>last month they opened the door, left the parcel and walked away after ringing the doorbell to get my attention. today he tried the front door to establish whether it wa locked or not, when it wa unlocked, he closed it, rang the bell and passed the parcel to me! i didn’t say anything a i can’t deal with confrontation so i have been onto the the live chat section to complain. having amazon driver thinking that it is ok to open my front door to leave package rather than ring the bell like they are supposed to. this ha now happened two month in a row</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.990396</td>\n",
       "      <td>9.603965e-03</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ordered from uber eats for the first time. my food didn’t arrive. i check the app and the the guy delivering my food claim he wa waiting outside my house for 8 minute but in that time never once rang my doorbell. now i’m down £15 and they won’t refund my order.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.989015</td>\n",
       "      <td>1.098530e-02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>getting mildly excited buying a new vacuum cleaner head and realising you’ve reached peak middle age at 34.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.986069</td>\n",
       "      <td>1.393138e-02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>pensioner going to the butcher's on a saturday morning. you've had all week, clive. i get 3 hour on a saturday morning and here you are adding to the already massive queue and dithering at the counter when you finally get there.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Success      Problems  Pred Value  True Value  \\\n",
       "75   1.000000  1.414174e-10  0           1            \n",
       "428  0.999975  2.498114e-05  0           1            \n",
       "360  0.999623  3.765296e-04  0           1            \n",
       "629  0.998978  1.021585e-03  0           1            \n",
       "605  0.995229  4.770634e-03  0           1            \n",
       "457  0.995215  4.785398e-03  0           1            \n",
       "660  0.993099  6.901117e-03  0           1            \n",
       "45   0.990396  9.603965e-03  0           1            \n",
       "74   0.989015  1.098530e-02  0           1            \n",
       "184  0.986069  1.393138e-02  0           1            \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Content  \n",
       "75   we bought our first house together last month. the wife hated the carpet and wall in the main bedroom but our work holiday diary were full. so for the pas two weeks, i've ripped up the old carpet and the laminate flooring below it, got an electrician to put in three double plug socket with usb point a there wa just a single plug socket in the whole room, repainted every wall white with four coat (they were all pink before). and reprinted the shirting-boards. all mainly on my hour lunch break while i'm working from home. we also needed a plasticer to fill in the hole left from the new plug sockets. we couldn't find a qualified plasticer so the wife got some training at work from a plasticer (she work on a building site.) and she did at the plasticing a couple of weekend ago. we were just replacing the curtain pole when the wife spotted a leak. she wa just out the shower. it look like there is a leak in the en-suite shower room. i did get the week off this week so i can decorate the dinner room, hallway and living room with the carpet fixer coming monday morning 9am to fix the new carpet in the bedroom. we, just, managed to get hold of the carpet fitter to postpone it a few week and the insurance company. both closed at 4pm on a sunday. we discovered the leak at 345pm. now a plumber coming instead of carpet fitter and i'm going to have to re-decorate half of that bedroom again once it's all be sorted. better go to b&amp;q for more paint. spending lunchtime for the past two week (and two weekend in a row) decorating the main bedroom. finishing it on the sunday afternoon before the carpet fitter come monday... just found a leak!!!  \n",
       "428  it get hoovered several time a week. is this some late-stage capitalism insanity, or am i just out of the loop? the sun is finally out after a wet saturday morning, perfect to enjoy some time in the garden, to the maddening soundtrack of next door hoovering their artificial grass.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "360  for my sims i watch a certain breakfast to show, and it seems like near enough every week there is a “these award ceremony went on last night, let’s look at who won, what they won, what they won it for, what they wore, the party they went to, what they said, etc” &amp;#x200b; i really don’t care, i will watch your tv programmes, but couldn’t give a crap about the award you then win, or who should have won instead not caring one bit about “award shows”                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "629  there wa one small piece of sausage in the whole slice! one! would not have again saw a post yesterday about vegan greggs product with great reviews, thought i'd try the vegan sausage cheesz and bean slice - wa massively disappointed!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "605  the guy picked it up with a bag, but the smell still reached me at the other end of the carriage. a dog just took a shit on the floor of the overground. happy monday morning..!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "457  unfortunately waterstones ship their book in cardboard so i now have a signed first edition of a book no longer available to buy with run ink and damaged page getting home after a week to find that, instead of taking an undelivered item to the sorting office, the postman decided to drop the clearly labelled waterstones package over my back fence for it to sit in the rain in the flower bed.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "660  last month they opened the door, left the parcel and walked away after ringing the doorbell to get my attention. today he tried the front door to establish whether it wa locked or not, when it wa unlocked, he closed it, rang the bell and passed the parcel to me! i didn’t say anything a i can’t deal with confrontation so i have been onto the the live chat section to complain. having amazon driver thinking that it is ok to open my front door to leave package rather than ring the bell like they are supposed to. this ha now happened two month in a row                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "45   ordered from uber eats for the first time. my food didn’t arrive. i check the app and the the guy delivering my food claim he wa waiting outside my house for 8 minute but in that time never once rang my doorbell. now i’m down £15 and they won’t refund my order.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "74   getting mildly excited buying a new vacuum cleaner head and realising you’ve reached peak middle age at 34.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "184  pensioner going to the butcher's on a saturday morning. you've had all week, clive. i get 3 hour on a saturday morning and here you are adding to the already massive queue and dithering at the counter when you finally get there.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_predictions.sort_values(by='Success', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d17e20",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eaa507",
   "metadata": {},
   "source": [
    "**Summmary**<br>\n",
    "In conclusion, the best model for me is the Lemmatization and CountVectorizer x Multinomial Naive Bayes. It gave good accuracy score of **79%**, which is higher than the bestline of 46%. From a misclassification perspective, it has the best F1 score and a good balance of false positives and negatives. \n",
    "\n",
    "With these in mind, I can conclude that the model does fairly well in classifying the two sub-reddits and there are considerable differences between the two sub-reddits. To answer the problem statement, I could conclude with a fair amount of confidence that setting up SingaporeanSuccess and SingaporeanProblem sub-reddits at the same time would not cannabalise viewers based on the topics itself. \n",
    "\n",
    "**Limitations**<br>\n",
    "However, as pointed out, there are limitations with my model in terms of the size of that data. In total, there were only 2.6k posts in this dataset, and with most of them coming from the recent month. It will be better if I have access to more data of a longer time period so as to widen the scope of the words for the model to train on (for example, our model is trained on words popular in the recent time period - e.g. Euros, crane flies)\n",
    "\n",
    "Similarly, sarcasm/irony are probably better analysed in the context of the sentence itself. As my misclassification analysis have pointed out, word vectorization have a good chance of missing the nuances used in ironic language. Sentence embedding with newer models such as BERT could potentially be helpful in analysing humour.\n",
    "\n",
    "Bring this back to our problem statement, I am also not 100% confident that this will also work as well for Singaporean humour, which by itself is also a very dynamic language with its own linguistic structures. For example, will the model be able to predict the nuances between 'can *lah*', 'can *leh*', can *lor*'? This will be something interesting to be explored further.\n",
    "\n",
    "Good luck!\n",
    "<img src=\"https://blog.spjain.org/wp-content/muploads/2011/06/LAh-2.jpg\" style=\"float: center; margin: 20px; height: 400px\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "278.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
